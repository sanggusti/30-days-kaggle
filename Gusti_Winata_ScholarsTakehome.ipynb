{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanggusti/30-days-kaggle/blob/main/Gusti_Winata_ScholarsTakehome.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d982b06b-67a4-4648-ac77-e554bb24dc7b",
      "metadata": {
        "id": "d982b06b-67a4-4648-ac77-e554bb24dc7b"
      },
      "source": [
        "# Cohere Labs Scholars 2Ô∏è‚É£0Ô∏è‚É£2Ô∏è‚É£6Ô∏è‚É£ : TakeHome Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93f94f3b-5d88-4708-88df-2df291d2a428",
      "metadata": {
        "id": "93f94f3b-5d88-4708-88df-2df291d2a428"
      },
      "source": [
        "# **Background**\n",
        "\n",
        "Welcome to the Cohere Labs Scholars Program Take-Home Challenge! This exercise is designed to allow you to showcase your engineering and problem solving skills. The Assessment consists of different challenges including:\n",
        "\n",
        "*   Identifying bugs, and getting the code working. This is designed to test your ability to grapple with real world engineering challenges.\n",
        "*   Testing your ability to generate code for a specified problem.\n",
        "*   An opportunity for you to attempt an optional challenge question that extends the original problem set.\n",
        "\n",
        "These tasks were chosen as a setting to see how you think about problems, even if they are not in your own research field of interest. The tasks and dataset are not meant to be indicative of the research goals of the Scholar Program. We purposefully have selected a simple toy problem so the focus is on how you think, and does not require significant machine learning resources (can be run in this colab).\n",
        "\n",
        "Good luck! üçÄ\n",
        "\n",
        "**How to Use and Submit this Document?**\n",
        "\n",
        "*   Make a copy of this document and rename it **Firstname_Lastname_ScholarsTakehome**\n",
        "* Once you have completed all tasks:\n",
        "  * Save and pin your revisions\n",
        "  * Download the colab as a .ipynb file\n",
        "  * Submit the assignment via the submission link you received via email (subject line: \"Cohere Labs: Research Scholar Program - Next Steps\") by **September 16 by 11pm PDT**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b6fd487-0e99-4912-a66d-d5c2641dee6f",
      "metadata": {
        "id": "4b6fd487-0e99-4912-a66d-d5c2641dee6f"
      },
      "source": [
        "### This Coding Challenge(üö® 25 points) consists of 4 parts :\n",
        "\n",
        "1. **Debugging custom SmolMoELM** üîçüêõ[*10 points*]\n",
        "2. **Upcycling a Dense Model into an MoE üîÑ üö¥** [*3 points*]\n",
        "3. **Continued Pretraining üìöüí™** [*7 points*]\n",
        "4. **Exploring The Unknown üßô ‚ú®** [*5 points*]\n",
        "\n",
        "Each of these build on top of each other so you are encouraged to work through them in order.\n",
        "\n",
        "**NOTE**: Part 4 can also be attempted independently(*if you don't wish to build on the previous section*)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02ad4c13-cc4d-4078-aa8b-ae38df57e8db",
      "metadata": {
        "id": "02ad4c13-cc4d-4078-aa8b-ae38df57e8db"
      },
      "source": [
        "## **Coding Challenge Part 1: Debugging custom Smol`MoE`LM üîçüêõ [üö® 10 points]**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "670cd318-7de4-4aa5-ae78-ccbc949b21a7",
      "metadata": {
        "id": "670cd318-7de4-4aa5-ae78-ccbc949b21a7"
      },
      "source": [
        "**Mixture of Experts (MoE)** are all the rage in 2025, powering some of the most advanced large-scale AI systems. In this coding challenge, you are required to dive into the core idea behind MoE and fix a bare-bones implementation.\n",
        "\n",
        "We have **üö® 10 bugs** in the following implementation.\n",
        "There is a section `3.Test` for your convenience to verify you have correctly identified all the bugs(Both `Check #1` and `Check #2` will help you confirm this).\n",
        "\n",
        "**Rules**:\n",
        "1. **Bug Definition:**\n",
        "  - There are **üö® 10 bugs** to be fixed.\n",
        "  - A bug is *defined as **{incorrect, missing, unnecessary}** lines of code*.\n",
        "  - You earn 1 point for each correctly identified and fixed bug.\n",
        "2. **Fix Guidelines:**\n",
        "  - You are encouraged to make the smallest possible fix, wherever possible (e.g. edit a line instead of replacing it entirely).\n",
        "  - Do not optimize the code in any way (combine functions, change variable names, etc) ; **only fix the bugs**. The implementation is *intentionally* non-optimized but valid.\n",
        "  - **Note:** Some bugs may require more than one line of correction/addition.\n",
        "\n",
        "3. **Documentation:** Document each fix by adding a comment on the line above the fix: : `### BUG FIX ###`.\n",
        "4. **Sections:** *1. Setup [Helper Functions]* and *3. Test* don't contain bugs and shouldn't be changed.\n",
        "5. **Multiple Bug Fixes:** Do not worry about possibly solving multiple bugs with a single fix. Should that rare case arise, you will still be awarded with the correct number of points as long as the fixes are the only changes made.\n",
        "6. **Rewriting the Implementation:** Rewriting the implementation to get around the bugs will not count towards any points. You are to strictly work within the implementation extending/modifying it only as much as required(indicated by *Step 3*)\n",
        "7. **Submission:** Your final submission should be the exact same notebook except with your proposed fixes in the cells and the respective comments as per Rule #3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b129e0c-8489-45cb-b411-b353df684943",
      "metadata": {
        "id": "4b129e0c-8489-45cb-b411-b353df684943"
      },
      "outputs": [],
      "source": [
        "# Example of a bug fix\n",
        "\n",
        "def _calc_square_root(x):\n",
        "    ### BUG FIX ###\n",
        "    # ans = x*2\n",
        "    ans = x**(1/2)\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87bf0e41-67a4-4342-9207-b731848d1257",
      "metadata": {
        "id": "87bf0e41-67a4-4342-9207-b731848d1257"
      },
      "source": [
        "### 1. Setup [Helper Functions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e8496e0e-3eee-4e7a-b153-1df291fdb61d",
      "metadata": {
        "id": "e8496e0e-3eee-4e7a-b153-1df291fdb61d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "679385ad459740cc8fd85deaa3a01770",
            "5bf759f6959146af9e007269590b32cc",
            "6c57e5e68f074c32aa5d694d780b54c8",
            "99ceee44faf4434fa23fd46e22d4b003",
            "07e854d90ad94c5e85ab7efc6e8a36e6",
            "65df1c9dddbf4665b43d477f26c17cf4",
            "6b829aeec9314d95b3de85cf23fc77f2",
            "350d036fe5624c08966d85824e602718",
            "61849b775da848e9ae172c1f12c13101",
            "b8929d71b71c4603b28d4d16aa14e3e6",
            "0ed503eaad1049fc95b26a373fcacf56"
          ]
        },
        "outputId": "91ec12cd-a0ac-4b99-e053-bfcaab52c7ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "trial_weights.pt:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "679385ad459740cc8fd85deaa3a01770"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "######################################################################################################################\n",
        "############################################## DO NOT CHANGE[START] ##################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# # Download the weights from HF\n",
        "from huggingface_hub import hf_hub_download\n",
        "path = hf_hub_download(repo_id=\"dsouzadaniel/C4AI_SmolMoELM\",\n",
        "                       filename=\"trial_weights.pt\",\n",
        "                      local_dir=\".\",)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6aa85481-6545-4a61-bc99-c06b54fe2da5",
      "metadata": {
        "id": "6aa85481-6545-4a61-bc99-c06b54fe2da5"
      },
      "outputs": [],
      "source": [
        "# Libraries\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "def timed(fn):\n",
        "    '''Simple Timing Decorator'''\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.perf_counter()\n",
        "        out = fn(*args, **kwargs)\n",
        "        total_time = time.perf_counter() - start_time\n",
        "        print(f\"time={total_time:.3f}s\")\n",
        "        return out\n",
        "    return wrapper\n",
        "\n",
        "def labelthis(label):\n",
        "    '''Simple Label Assigner'''\n",
        "    def deco(fn):\n",
        "        fn.label = label\n",
        "        return fn\n",
        "    return deco\n",
        "\n",
        "def pretty_dt(s: float) -> str:\n",
        "    '''Print Time Taken(but pretty :) )'''\n",
        "    if s < 1e-6: return f\"{s*1e9:.0f} ns\"\n",
        "    if s < 1e-3: return f\"{s*1e6:.0f} ¬µs\"\n",
        "    if s < 1:    return f\"{s*1e3:.0f} ms\"\n",
        "    if s < 60:   return f\"{s:.3f} s\"\n",
        "    h, s = divmod(s, 3600); m, s = divmod(s, 60)\n",
        "    return (f\"{int(m)}m {int(s)}s\" if h < 1 else f\"{int(h)}h {int(m)}m {int(s)}s\")\n",
        "\n",
        "@timed\n",
        "def __generate(model, tokenizer, inputs, num_tokens):\n",
        "    '''Helper function. Recommended to use via `generation_compare`'''\n",
        "    collect = []\n",
        "    for _ in range(num_tokens):\n",
        "        output = model(**inputs)\n",
        "        output_id = torch.argmax(output['logits'][0,-1]).item()\n",
        "        collect.append(output_id)\n",
        "        if output_id==tokenizer.eos_token_id:\n",
        "            break\n",
        "        inputs['input_ids'] = torch.unsqueeze(torch.cat([inputs['input_ids'][0],torch.tensor([output_id])]),dim=0)\n",
        "        inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])\n",
        "    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(collect))\n",
        "\n",
        "def generation_compare(prompt, num_tokens, tokenizer, model_A, model_B=None):\n",
        "    '''Compares generations of two models. Passing just one model provides simple generation utility'''\n",
        "    print()\n",
        "    print(f\"{'>'*20}\\n\\tPrompt\\n{'<'*20}\\n{prompt}\\n\\n\")\n",
        "    model_inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    print(f\"{'>'*30}\\n\\tModel_A Generation\\n{'<'*30}\\n{__generate(model_A,  tokenizer, model_inputs, num_tokens)}\")\n",
        "    print(\"\\n\\n\")\n",
        "    if model_B:\n",
        "        model_inputs = tokenizer(prompt, return_tensors='pt')\n",
        "        print(f\"{'>'*30}\\n\\tModel_B Generation\\n{'<'*30}\\n{__generate(model_B,  tokenizer, model_inputs, num_tokens)}\")\n",
        "\n",
        "def detach_metrics(metrics: dict):\n",
        "    '''helper for metrics'''\n",
        "    def to_cpu(x):\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            # If scalar, return float; if vector/matrix, return list\n",
        "            return x.detach().cpu().item() if x.dim() == 0 else x.detach().cpu().tolist()\n",
        "        elif isinstance(x, list):\n",
        "            return [to_cpu(y) for y in x]\n",
        "        elif isinstance(x, dict):\n",
        "            return {k: to_cpu(v) for k, v in x.items()}\n",
        "        return x\n",
        "\n",
        "    return {k: to_cpu(v) for k, v in metrics.items()}\n",
        "\n",
        "def plot_metrics(metrics: dict, x_vals=None, suptitle=\"Training Metrics\"):\n",
        "    '''For grid plotting a collection of metrics'''\n",
        "    metrics = detach_metrics(metrics)\n",
        "\n",
        "    keys = list(metrics.keys())\n",
        "    n = len(keys)\n",
        "    length = len(next(iter(metrics.values())))\n",
        "    if not x_vals:\n",
        "        x_vals = list(range(1,length + 1))\n",
        "\n",
        "    fig, axes = plt.subplots(1, n, figsize=(4*n, 3), constrained_layout=True)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    palette = plt.cm.tab10.colors\n",
        "\n",
        "    for i, (ax, key_str) in enumerate(zip(axes, keys)):\n",
        "        y_vals = metrics[key_str]\n",
        "        ax.plot(x_vals, y_vals, marker=\"o\", color=palette[i % len(palette)])\n",
        "        ax.set_title(key_str)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    fig.suptitle(suptitle)\n",
        "    fig.supxlabel(\"Steps\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class smolMoEConfig:\n",
        "    vocab_size=49152\n",
        "    hidden_size=576\n",
        "    intermediate_size=1536\n",
        "    num_hidden_layers = 30\n",
        "    num_heads=9\n",
        "    kv_heads=3\n",
        "    num_experts = 3\n",
        "    num_experts_per_tok = 1\n",
        "\n",
        "config = smolMoEConfig\n",
        "\n",
        "TEST_PROMPT = \"Where is the Great Wall?\"\n",
        "\n",
        "######################################################################################################################\n",
        "############################################### DO NOT CHANGE[END] ###################################################\n",
        "######################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4333f0eb-1f9c-4536-8ec6-49b841467d26",
      "metadata": {
        "id": "4333f0eb-1f9c-4536-8ec6-49b841467d26"
      },
      "source": [
        "### 2. Custom Smol`MoE`LM (for BugFixes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bug found:\n",
        "\n",
        "1. RotaryEmbedder -> Wrong type.\n",
        "Changes:\n",
        "```\n",
        "-- self.freq = 1/(base ** (torch.arange(0, dim, 2, dtype=torch.int64).float()/dim))\n",
        "++ self.freq = 1.0/(base ** (torch.arange(0, dim, 2, dtype=torch.float32) /dim))\n",
        "```\n",
        "2. MoE initialization -> The model has gate, not route.\n",
        "Changes:\n",
        "```\n",
        "-- self.router = nn.Linear(self.D, self.k, bias=False, dtype=dtype)\n",
        "++ self.gate = nn.Linear(self.D, self.E, bias=False, dtype=dtype)\n",
        "```\n",
        "3. MoE Expert Utilization Function -> Load Balancer Loss formula.\n",
        "Changes\n",
        "```\n",
        "-- load = torch.mean(selected.float(), dim=(0,1))\n",
        "-- self._aux_lb = self.E * load.sum()\n",
        "-- self._expert_utilization = selected\n",
        "\n",
        "++ probs = F.softmax(logits, dim=-1)\n",
        "++ load = torch.mean(selected.float(), dim=(0,1))\n",
        "++ density_proxy = torch.mean(probs, dim=(0,1))\n",
        "++ self._aux_lb = (self.E**2) * torch.mean(density_proxy * load)\n",
        "++ self._expert_utilization = selected\n",
        "```\n",
        "4. MoE Forward method -> not router, but gate.\n",
        "Changes\n",
        "```\n",
        "-- logits = self.router(x)\n",
        "++ logits = self.gate(x)\n",
        "```\n",
        "5. MoE Forward method -> Swish need to be `F.silu(a) * u` and einsum for y is `\"bteh,ehd->bted\"`. Changes\n",
        "```\n",
        "-- h = F.silu(u)\n",
        "-- y = torch.einsum(\"bteh,ehd->bteh\", h, self.down_bank)\n",
        "++ h = F.silu(a) * u\n",
        "++ y = torch.einsum(\"bteh,ehd->bted\", h, self.down_bank)\n",
        "```\n",
        "6. RopeAttention -> `rotary_emb` redundant calling dim since it's already defined as `head_dim`. Changes\n",
        "```\n",
        "-- self.rotary_emb = RotaryEmbedder(base=self.rope_theta,\n",
        "                                    dim=config.hidden_size//self.num_heads)\n",
        "++ self.rotary_emb = RotaryEmbedder(base=self.rope_theta,\n",
        "                                    dim=self.head_dim)\n",
        "```\n",
        "7. RopeAttention forward method -> the one rotates should be `q_states`, not `v_states`. Changes\n",
        "```\n",
        "-- cos, sin = self.rotary_emb(v_states)\n",
        "++ cos, sin = self.rotary_emb(q_states)\n",
        "```\n",
        "8. smolMoeModel forward -> it should return `hidden_states` variables, not in an array list. Changes\n",
        "```\n",
        "-- return [hidden_states]\n",
        "++ return hidden_states\n",
        "```\n",
        "9. smolMoeLM forward -> `hidden_states` should be taking whole outputs, not only the first index. Changes\n",
        "```\n",
        "-- hidden_states = outputs[0]\n",
        "++ hidden_states = outputs\n",
        "```\n",
        "10. Missing implementation of `smolMoELM.get_expert_utilization`. Changes\n",
        "```\n",
        "-- lb_loss, expert_utilization_per_layer = 0, 0\n",
        "-- return expert_utilization_per_layer, lb_loss\n",
        "\n",
        "++ expert_utils = []\n",
        "++ lb_losses = []\n",
        "++ for layer in self.model.layers:\n",
        "++     moe = getattr(layer, \"moe\", None)\n",
        "++     if moe is not None and hasattr(moe, \"_expert_utilization\") and hasattr(moe, \"_aux_lb\"):\n",
        "++        # (_expert_utilization) shape: (B,T,E) one-hot per token\n",
        "++        util = moe._expert_utilization.float().mean(dim=(0,1))  # (E,)\n",
        "++        expert_utils.append(util)\n",
        "++        lb_losses.append(moe._aux_lb)\n",
        "++ if expert_utils:\n",
        "++     expert_utilization_per_layer = torch.stack(expert_utils, dim=0)  # (L,E)\n",
        "++     lb_loss = torch.stack(lb_losses).mean()\n",
        "++ else:\n",
        "++     expert_utilization_per_layer = torch.tensor(0.)\n",
        "++     lb_loss = torch.tensor(0.)\n",
        "++ return expert_utilization_per_layer, lb_loss\n",
        "```\n",
        "\n",
        "With these important fixes, the model are able to generate coherent text.\n",
        "\n",
        "```\n",
        ">>>>>>>>>>>>>>>>>>>>\n",
        "\tPrompt\n",
        "<<<<<<<<<<<<<<<<<<<<\n",
        "Where is the Great Wall?\n",
        "\n",
        "\n",
        "time=9.400s\n",
        ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "\tModel_A Generation\n",
        "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "\n",
        "The Great Wall of China is a 13,000-mile-long wall that spans 13,000 miles from the Yellow River in Shaanxi province in China to the border with Mongolia. It is\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "and the other check of `lb_loss` also passed\n",
        "\n",
        "```\n",
        "(Expected) Load Balance Loss => 1.00\n",
        "(Actual) Load Balance Loss => 1.00\n",
        "```"
      ],
      "metadata": {
        "id": "lc01l4sTcGyO"
      },
      "id": "lc01l4sTcGyO"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "a48e891f-08f6-4b6f-be00-fa31e3fd3005",
      "metadata": {
        "id": "a48e891f-08f6-4b6f-be00-fa31e3fd3005"
      },
      "outputs": [],
      "source": [
        "def rotate_half(x):\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
        "    if cos.device != q.device:\n",
        "        cos = cos.to(q.device)\n",
        "        sin = sin.to(q.device)\n",
        "    cos = cos.unsqueeze(unsqueeze_dim)\n",
        "    sin = sin.unsqueeze(unsqueeze_dim)\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "def repeat_kv(hidden_states, n_rep):\n",
        "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
        "\n",
        "class RotaryEmbedder(nn.Module):\n",
        "    def __init__(self, dim, base):\n",
        "        super().__init__()\n",
        "        ### BUG FIX 1 ###\n",
        "        self.freq = 1.0/(base ** (torch.arange(0, dim, 2, dtype=torch.float32) /dim))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self,x):\n",
        "        # x : (B,H,T,D)\n",
        "        pos = torch.arange(x.shape[-2], dtype=torch.long)\n",
        "        angles = torch.einsum('p,f->pf', pos.float(), self.freq).unsqueeze(dim=0)\n",
        "        # (B,T,dim)\n",
        "        emb = torch.cat((angles, angles), dim=-1)\n",
        "        return emb.cos(), emb.sin()\n",
        "\n",
        "class MoE(nn.Module):\n",
        "    \"\"\"\n",
        "    An MoE layer with MLP block with swiglue activation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_experts_per_tok: int, num_experts: int, emb_dim: int, moe_dim: int, dtype=torch.float32):\n",
        "        super().__init__()\n",
        "        self.k = int(num_experts_per_tok)\n",
        "        self.E = int(num_experts)\n",
        "        self.D = int(emb_dim)\n",
        "        self.H = int(moe_dim)\n",
        "\n",
        "        ### BUG FIX 2 ###\n",
        "        # 2 wrong router output size, logits (B,T,E) but here is (B,T,k) and it was gate, not router, and bias is not available in the weights\n",
        "        self.gate = nn.Linear(self.D, self.E, bias=False, dtype=dtype)\n",
        "        self.gate_bank = nn.Parameter(torch.empty(self.E, self.D, self.H, dtype=dtype))\n",
        "        self.up_bank   = nn.Parameter(torch.empty(self.E, self.D, self.H, dtype=dtype))\n",
        "        self.down_bank = nn.Parameter(torch.empty(self.E, self.H, self.D, dtype=dtype))\n",
        "\n",
        "    def expert_utilization(self, logits):\n",
        "        \"\"\"\n",
        "        This function compute expert utilization per token and also compute load balancer loss.\n",
        "        Details of this load balancer can be found in https://arxiv.org/abs/2101.03961\n",
        "        \"\"\"\n",
        "        selected = torch.argmax(logits, dim=-1)\n",
        "        selected = F.one_hot(selected, num_classes=self.E)\n",
        "\n",
        "        ### BUG FIX 3 ###\n",
        "        # Based on paper, intended loss in Switch-Transformer uses both the actual fraction routed and\n",
        "        # probability mass assigned by the router and computes a dot product term scaled by num_experts**2\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        load = torch.mean(selected.float(), dim=(0,1))\n",
        "        density_proxy = torch.mean(probs, dim=(0,1))\n",
        "        self._aux_lb = (self.E ** 2) * torch.mean(density_proxy * load)\n",
        "\n",
        "        self._expert_utilization = selected\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, D = x.shape\n",
        "        assert D == self.D, f\"Expected emb_dim={self.D}, got {D}\"\n",
        "\n",
        "        ### BUG FIX 4 ###\n",
        "        logits = self.gate(x)\n",
        "\n",
        "        if self.training:\n",
        "            logits = logits + torch.randn_like(logits) * 1e-1\n",
        "\n",
        "        selected = torch.argmax(logits, dim=-1)\n",
        "        a = torch.einsum(\"btd,edh->bteh\", x, self.gate_bank)\n",
        "        u = torch.einsum(\"btd,edh->bteh\", x, self.up_bank)\n",
        "        ### BUG FIX 5 ###\n",
        "        h = F.silu(a) * u\n",
        "        ### BUG FIX 6 ###\n",
        "        y = torch.einsum(\"bteh,ehd->bted\", h, self.down_bank)\n",
        "\n",
        "        gather_idx = selected.view(B,T,1,1).expand(-1, -1, -1, D)\n",
        "        y = torch.gather(y, dim=2, index=gather_idx).squeeze(-2)\n",
        "\n",
        "        self.expert_utilization(logits)\n",
        "        return y\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.variance_epsilon = eps\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
        "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
        "        return self.weight * hidden_states\n",
        "\n",
        "\n",
        "class RopeAttention(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_dim = config.hidden_size // self.num_heads\n",
        "        self.kv_heads = config.kv_heads\n",
        "        self.rope_theta = 10000.0\n",
        "\n",
        "        self.W_query = nn.Linear(config.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
        "        self.W_key = nn.Linear(config.hidden_size, self.kv_heads * self.head_dim, bias=False)\n",
        "        self.W_value = nn.Linear(config.hidden_size, self.kv_heads * self.head_dim, bias=False)\n",
        "        self.W_output = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n",
        "\n",
        "        ### BUG FIX 7 ###\n",
        "        self.rotary_emb = RotaryEmbedder(base=self.rope_theta,\n",
        "                                         dim=self.head_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask= None,\n",
        "    ):\n",
        "        b, q, _ = hidden_states.size()\n",
        "\n",
        "        q_states = self.W_query(hidden_states)\n",
        "        k_states = self.W_key(hidden_states)\n",
        "        v_states = self.W_value(hidden_states)\n",
        "\n",
        "        q_states = q_states.view(b, q, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k_states = k_states.view(b, q, self.kv_heads, self.head_dim).transpose(1, 2)\n",
        "        v_states = v_states.view(b, q, self.kv_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        ### BUG FIX 8 ###\n",
        "        cos, sin = self.rotary_emb(q_states)\n",
        "        q_states, k_states = apply_rotary_pos_emb(q_states, k_states, cos, sin)\n",
        "\n",
        "        __kv_groups = self.num_heads // self.kv_heads\n",
        "\n",
        "        k_states = repeat_kv(k_states, __kv_groups)\n",
        "        v_states = repeat_kv(v_states, __kv_groups)\n",
        "\n",
        "        attn_weights = torch.matmul(q_states, k_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "        ### BUG FIX Attempt ###\n",
        "        if attention_mask is not None:\n",
        "            # Ensure mask is (B, 1, T, T) for broadcasting\n",
        "            if attention_mask.dim() == 2:  # (B, T) -> (B, 1, T, T)\n",
        "                attention_mask = attention_mask[:, None, :, None].expand(-1, 1, -1, attention_mask.size(-1))\n",
        "            attn_weights = attn_weights + attention_mask\n",
        "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
        "        attn_weights = nn.functional.dropout(attn_weights,p=0)\n",
        "\n",
        "\n",
        "        attn_output = torch.matmul(attn_weights, v_states)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.reshape(b, q, -1)\n",
        "\n",
        "        attn_output = self.W_output(attn_output)\n",
        "\n",
        "        return attn_output\n",
        "\n",
        "class LlamaDecoder(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.self_attn = RopeAttention(config)\n",
        "        self.moe = MoE(num_experts=config.num_experts,\n",
        "                       num_experts_per_tok=config.num_experts_per_tok,\n",
        "                       emb_dim=config.hidden_size,\n",
        "                       moe_dim=config.intermediate_size)\n",
        "        self.pre_attn_rmsnorm = RMSNorm(config.hidden_size, eps=1e-05)\n",
        "        self.pre_moe_rmsnorm = RMSNorm(config.hidden_size, eps=1e-05)\n",
        "\n",
        "    def forward(self,hidden_states, attention_mask):\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.pre_attn_rmsnorm(hidden_states)\n",
        "        ### BUG FIX Attempt ###\n",
        "        seq_len = attention_mask.shape[-1]\n",
        "        attention_mask = torch.triu(torch.full((seq_len, seq_len), fill_value=float('-inf'), device = hidden_states.device),diagonal=1)\n",
        "        attention_mask = attention_mask.unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "        hidden_states = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        hidden_states += residual\n",
        "        residual = hidden_states\n",
        "\n",
        "        hidden_states = self.pre_moe_rmsnorm(hidden_states)\n",
        "\n",
        "        # MLP block\n",
        "        hidden_states = self.moe(hidden_states)\n",
        "        hidden_states += residual\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class smolMoEModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embed_tokens = nn.Embedding(num_embeddings=config.vocab_size,\n",
        "                                         embedding_dim=config.hidden_size)\n",
        "        self.layers = nn.ModuleList([\n",
        "            LlamaDecoder(config) for _ in range(config.num_hidden_layers)\n",
        "            ])\n",
        "        self.norm = RMSNorm(config.hidden_size, eps=1e-05)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids= None,\n",
        "        attention_mask= None,\n",
        "    ):\n",
        "        inputs_embeds = self.embed_tokens(input_ids)\n",
        "        hidden_states = inputs_embeds\n",
        "        for decoder_layer in self.layers:\n",
        "            layer_outputs = decoder_layer(\n",
        "                hidden_states,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "        hidden_states = self.norm(hidden_states)\n",
        "\n",
        "        ### BUG FIX ###\n",
        "        return hidden_states\n",
        "\n",
        "class smolMoELM(nn.Module):\n",
        "    def __init__(self,config):\n",
        "        super().__init__()\n",
        "        self.model = smolMoEModel(config)\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "        self.tie_weights()\n",
        "\n",
        "    def tie_weights(self):\n",
        "        self.lm_head.weight = self.model.embed_tokens.weight\n",
        "\n",
        "    def forward(self,input_ids,attention_mask):\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        hidden_states = outputs\n",
        "\n",
        "        logits = self.lm_head(hidden_states)\n",
        "        logits = logits.float()\n",
        "        return {'logits':logits}\n",
        "\n",
        "    def get_expert_utilization(self):\n",
        "        ### BUG FIX ###\n",
        "        # 9 Previously returns 0,0 all the time, need to aggregate expert utilization & load balance loss across layers\n",
        "        expert_utils = []\n",
        "        lb_losses = []\n",
        "\n",
        "        for layer in self.model.layers:\n",
        "            moe = getattr(layer, \"moe\", None)\n",
        "            if moe is not None and hasattr(moe, \"_expert_utilization\") and hasattr(moe, \"_aux_lb\"):\n",
        "                # (_expert_utilization) shape: (B,T,E) one-hot per token\n",
        "                util = moe._expert_utilization.float().mean(dim=(0,1))  # (E,)\n",
        "                expert_utils.append(util)\n",
        "                lb_losses.append(moe._aux_lb)\n",
        "\n",
        "        if expert_utils:\n",
        "            expert_utilization_per_layer = torch.stack(expert_utils, dim=0)  # (L,E)\n",
        "            lb_loss = torch.stack(lb_losses).mean()\n",
        "        else:\n",
        "            expert_utilization_per_layer = torch.tensor(0.)\n",
        "            lb_loss = torch.tensor(0.)\n",
        "\n",
        "        return expert_utilization_per_layer, lb_loss\n",
        "\n",
        "    def reset_weights_and_metrics(self):\n",
        "        with torch.no_grad():\n",
        "            modules = list(self.modules())[1:]\n",
        "            for m in modules:\n",
        "                fn = getattr(m, \"reset_parameters_\", None) or getattr(m, \"reset_parameters\", None)\n",
        "                if callable(fn):\n",
        "                    fn()\n",
        "\n",
        "            for m in modules:\n",
        "                if hasattr(m, \"reset_parameters\") or hasattr(m, \"reset_parameters_\"):\n",
        "                    continue\n",
        "                any_param = False\n",
        "                for name, p in m.named_parameters(recurse=False):\n",
        "                    any_param = True\n",
        "                    if p.dim() == 1:\n",
        "                        if name == \"bias\":\n",
        "                            p.zero_()\n",
        "                        else:\n",
        "                            p.fill_(1.0)\n",
        "                    else:\n",
        "                        nn.init.kaiming_uniform_(p, a=math.sqrt(5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# personal testing cell\n",
        "\n",
        "checkpoint=\"HuggingFaceTB/SmolLM-135M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "config = smolMoEConfig\n",
        "\n",
        "model = smolMoELM(config)\n",
        "model.load_state_dict(torch.load('trial_weights.pt'), strict=True)\n",
        "model.eval()\n",
        "\n",
        "input_ids = torch.tensor([[1,2,3,4]])\n",
        "attention_mask = torch.ones(1,4)\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model(input_ids, attention_mask)\n"
      ],
      "metadata": {
        "id": "fiFJ__ROW8Wv"
      },
      "id": "fiFJ__ROW8Wv",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict().keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReycGe1zTbxH",
        "outputId": "5a76213b-78fe-4e35-b0a2-f799658c8457"
      },
      "id": "ReycGe1zTbxH",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['model.embed_tokens.weight', 'model.layers.0.self_attn.W_query.weight', 'model.layers.0.self_attn.W_key.weight', 'model.layers.0.self_attn.W_value.weight', 'model.layers.0.self_attn.W_output.weight', 'model.layers.0.moe.gate_bank', 'model.layers.0.moe.up_bank', 'model.layers.0.moe.down_bank', 'model.layers.0.moe.gate.weight', 'model.layers.0.pre_attn_rmsnorm.weight', 'model.layers.0.pre_moe_rmsnorm.weight', 'model.layers.1.self_attn.W_query.weight', 'model.layers.1.self_attn.W_key.weight', 'model.layers.1.self_attn.W_value.weight', 'model.layers.1.self_attn.W_output.weight', 'model.layers.1.moe.gate_bank', 'model.layers.1.moe.up_bank', 'model.layers.1.moe.down_bank', 'model.layers.1.moe.gate.weight', 'model.layers.1.pre_attn_rmsnorm.weight', 'model.layers.1.pre_moe_rmsnorm.weight', 'model.layers.2.self_attn.W_query.weight', 'model.layers.2.self_attn.W_key.weight', 'model.layers.2.self_attn.W_value.weight', 'model.layers.2.self_attn.W_output.weight', 'model.layers.2.moe.gate_bank', 'model.layers.2.moe.up_bank', 'model.layers.2.moe.down_bank', 'model.layers.2.moe.gate.weight', 'model.layers.2.pre_attn_rmsnorm.weight', 'model.layers.2.pre_moe_rmsnorm.weight', 'model.layers.3.self_attn.W_query.weight', 'model.layers.3.self_attn.W_key.weight', 'model.layers.3.self_attn.W_value.weight', 'model.layers.3.self_attn.W_output.weight', 'model.layers.3.moe.gate_bank', 'model.layers.3.moe.up_bank', 'model.layers.3.moe.down_bank', 'model.layers.3.moe.gate.weight', 'model.layers.3.pre_attn_rmsnorm.weight', 'model.layers.3.pre_moe_rmsnorm.weight', 'model.layers.4.self_attn.W_query.weight', 'model.layers.4.self_attn.W_key.weight', 'model.layers.4.self_attn.W_value.weight', 'model.layers.4.self_attn.W_output.weight', 'model.layers.4.moe.gate_bank', 'model.layers.4.moe.up_bank', 'model.layers.4.moe.down_bank', 'model.layers.4.moe.gate.weight', 'model.layers.4.pre_attn_rmsnorm.weight', 'model.layers.4.pre_moe_rmsnorm.weight', 'model.layers.5.self_attn.W_query.weight', 'model.layers.5.self_attn.W_key.weight', 'model.layers.5.self_attn.W_value.weight', 'model.layers.5.self_attn.W_output.weight', 'model.layers.5.moe.gate_bank', 'model.layers.5.moe.up_bank', 'model.layers.5.moe.down_bank', 'model.layers.5.moe.gate.weight', 'model.layers.5.pre_attn_rmsnorm.weight', 'model.layers.5.pre_moe_rmsnorm.weight', 'model.layers.6.self_attn.W_query.weight', 'model.layers.6.self_attn.W_key.weight', 'model.layers.6.self_attn.W_value.weight', 'model.layers.6.self_attn.W_output.weight', 'model.layers.6.moe.gate_bank', 'model.layers.6.moe.up_bank', 'model.layers.6.moe.down_bank', 'model.layers.6.moe.gate.weight', 'model.layers.6.pre_attn_rmsnorm.weight', 'model.layers.6.pre_moe_rmsnorm.weight', 'model.layers.7.self_attn.W_query.weight', 'model.layers.7.self_attn.W_key.weight', 'model.layers.7.self_attn.W_value.weight', 'model.layers.7.self_attn.W_output.weight', 'model.layers.7.moe.gate_bank', 'model.layers.7.moe.up_bank', 'model.layers.7.moe.down_bank', 'model.layers.7.moe.gate.weight', 'model.layers.7.pre_attn_rmsnorm.weight', 'model.layers.7.pre_moe_rmsnorm.weight', 'model.layers.8.self_attn.W_query.weight', 'model.layers.8.self_attn.W_key.weight', 'model.layers.8.self_attn.W_value.weight', 'model.layers.8.self_attn.W_output.weight', 'model.layers.8.moe.gate_bank', 'model.layers.8.moe.up_bank', 'model.layers.8.moe.down_bank', 'model.layers.8.moe.gate.weight', 'model.layers.8.pre_attn_rmsnorm.weight', 'model.layers.8.pre_moe_rmsnorm.weight', 'model.layers.9.self_attn.W_query.weight', 'model.layers.9.self_attn.W_key.weight', 'model.layers.9.self_attn.W_value.weight', 'model.layers.9.self_attn.W_output.weight', 'model.layers.9.moe.gate_bank', 'model.layers.9.moe.up_bank', 'model.layers.9.moe.down_bank', 'model.layers.9.moe.gate.weight', 'model.layers.9.pre_attn_rmsnorm.weight', 'model.layers.9.pre_moe_rmsnorm.weight', 'model.layers.10.self_attn.W_query.weight', 'model.layers.10.self_attn.W_key.weight', 'model.layers.10.self_attn.W_value.weight', 'model.layers.10.self_attn.W_output.weight', 'model.layers.10.moe.gate_bank', 'model.layers.10.moe.up_bank', 'model.layers.10.moe.down_bank', 'model.layers.10.moe.gate.weight', 'model.layers.10.pre_attn_rmsnorm.weight', 'model.layers.10.pre_moe_rmsnorm.weight', 'model.layers.11.self_attn.W_query.weight', 'model.layers.11.self_attn.W_key.weight', 'model.layers.11.self_attn.W_value.weight', 'model.layers.11.self_attn.W_output.weight', 'model.layers.11.moe.gate_bank', 'model.layers.11.moe.up_bank', 'model.layers.11.moe.down_bank', 'model.layers.11.moe.gate.weight', 'model.layers.11.pre_attn_rmsnorm.weight', 'model.layers.11.pre_moe_rmsnorm.weight', 'model.layers.12.self_attn.W_query.weight', 'model.layers.12.self_attn.W_key.weight', 'model.layers.12.self_attn.W_value.weight', 'model.layers.12.self_attn.W_output.weight', 'model.layers.12.moe.gate_bank', 'model.layers.12.moe.up_bank', 'model.layers.12.moe.down_bank', 'model.layers.12.moe.gate.weight', 'model.layers.12.pre_attn_rmsnorm.weight', 'model.layers.12.pre_moe_rmsnorm.weight', 'model.layers.13.self_attn.W_query.weight', 'model.layers.13.self_attn.W_key.weight', 'model.layers.13.self_attn.W_value.weight', 'model.layers.13.self_attn.W_output.weight', 'model.layers.13.moe.gate_bank', 'model.layers.13.moe.up_bank', 'model.layers.13.moe.down_bank', 'model.layers.13.moe.gate.weight', 'model.layers.13.pre_attn_rmsnorm.weight', 'model.layers.13.pre_moe_rmsnorm.weight', 'model.layers.14.self_attn.W_query.weight', 'model.layers.14.self_attn.W_key.weight', 'model.layers.14.self_attn.W_value.weight', 'model.layers.14.self_attn.W_output.weight', 'model.layers.14.moe.gate_bank', 'model.layers.14.moe.up_bank', 'model.layers.14.moe.down_bank', 'model.layers.14.moe.gate.weight', 'model.layers.14.pre_attn_rmsnorm.weight', 'model.layers.14.pre_moe_rmsnorm.weight', 'model.layers.15.self_attn.W_query.weight', 'model.layers.15.self_attn.W_key.weight', 'model.layers.15.self_attn.W_value.weight', 'model.layers.15.self_attn.W_output.weight', 'model.layers.15.moe.gate_bank', 'model.layers.15.moe.up_bank', 'model.layers.15.moe.down_bank', 'model.layers.15.moe.gate.weight', 'model.layers.15.pre_attn_rmsnorm.weight', 'model.layers.15.pre_moe_rmsnorm.weight', 'model.layers.16.self_attn.W_query.weight', 'model.layers.16.self_attn.W_key.weight', 'model.layers.16.self_attn.W_value.weight', 'model.layers.16.self_attn.W_output.weight', 'model.layers.16.moe.gate_bank', 'model.layers.16.moe.up_bank', 'model.layers.16.moe.down_bank', 'model.layers.16.moe.gate.weight', 'model.layers.16.pre_attn_rmsnorm.weight', 'model.layers.16.pre_moe_rmsnorm.weight', 'model.layers.17.self_attn.W_query.weight', 'model.layers.17.self_attn.W_key.weight', 'model.layers.17.self_attn.W_value.weight', 'model.layers.17.self_attn.W_output.weight', 'model.layers.17.moe.gate_bank', 'model.layers.17.moe.up_bank', 'model.layers.17.moe.down_bank', 'model.layers.17.moe.gate.weight', 'model.layers.17.pre_attn_rmsnorm.weight', 'model.layers.17.pre_moe_rmsnorm.weight', 'model.layers.18.self_attn.W_query.weight', 'model.layers.18.self_attn.W_key.weight', 'model.layers.18.self_attn.W_value.weight', 'model.layers.18.self_attn.W_output.weight', 'model.layers.18.moe.gate_bank', 'model.layers.18.moe.up_bank', 'model.layers.18.moe.down_bank', 'model.layers.18.moe.gate.weight', 'model.layers.18.pre_attn_rmsnorm.weight', 'model.layers.18.pre_moe_rmsnorm.weight', 'model.layers.19.self_attn.W_query.weight', 'model.layers.19.self_attn.W_key.weight', 'model.layers.19.self_attn.W_value.weight', 'model.layers.19.self_attn.W_output.weight', 'model.layers.19.moe.gate_bank', 'model.layers.19.moe.up_bank', 'model.layers.19.moe.down_bank', 'model.layers.19.moe.gate.weight', 'model.layers.19.pre_attn_rmsnorm.weight', 'model.layers.19.pre_moe_rmsnorm.weight', 'model.layers.20.self_attn.W_query.weight', 'model.layers.20.self_attn.W_key.weight', 'model.layers.20.self_attn.W_value.weight', 'model.layers.20.self_attn.W_output.weight', 'model.layers.20.moe.gate_bank', 'model.layers.20.moe.up_bank', 'model.layers.20.moe.down_bank', 'model.layers.20.moe.gate.weight', 'model.layers.20.pre_attn_rmsnorm.weight', 'model.layers.20.pre_moe_rmsnorm.weight', 'model.layers.21.self_attn.W_query.weight', 'model.layers.21.self_attn.W_key.weight', 'model.layers.21.self_attn.W_value.weight', 'model.layers.21.self_attn.W_output.weight', 'model.layers.21.moe.gate_bank', 'model.layers.21.moe.up_bank', 'model.layers.21.moe.down_bank', 'model.layers.21.moe.gate.weight', 'model.layers.21.pre_attn_rmsnorm.weight', 'model.layers.21.pre_moe_rmsnorm.weight', 'model.layers.22.self_attn.W_query.weight', 'model.layers.22.self_attn.W_key.weight', 'model.layers.22.self_attn.W_value.weight', 'model.layers.22.self_attn.W_output.weight', 'model.layers.22.moe.gate_bank', 'model.layers.22.moe.up_bank', 'model.layers.22.moe.down_bank', 'model.layers.22.moe.gate.weight', 'model.layers.22.pre_attn_rmsnorm.weight', 'model.layers.22.pre_moe_rmsnorm.weight', 'model.layers.23.self_attn.W_query.weight', 'model.layers.23.self_attn.W_key.weight', 'model.layers.23.self_attn.W_value.weight', 'model.layers.23.self_attn.W_output.weight', 'model.layers.23.moe.gate_bank', 'model.layers.23.moe.up_bank', 'model.layers.23.moe.down_bank', 'model.layers.23.moe.gate.weight', 'model.layers.23.pre_attn_rmsnorm.weight', 'model.layers.23.pre_moe_rmsnorm.weight', 'model.layers.24.self_attn.W_query.weight', 'model.layers.24.self_attn.W_key.weight', 'model.layers.24.self_attn.W_value.weight', 'model.layers.24.self_attn.W_output.weight', 'model.layers.24.moe.gate_bank', 'model.layers.24.moe.up_bank', 'model.layers.24.moe.down_bank', 'model.layers.24.moe.gate.weight', 'model.layers.24.pre_attn_rmsnorm.weight', 'model.layers.24.pre_moe_rmsnorm.weight', 'model.layers.25.self_attn.W_query.weight', 'model.layers.25.self_attn.W_key.weight', 'model.layers.25.self_attn.W_value.weight', 'model.layers.25.self_attn.W_output.weight', 'model.layers.25.moe.gate_bank', 'model.layers.25.moe.up_bank', 'model.layers.25.moe.down_bank', 'model.layers.25.moe.gate.weight', 'model.layers.25.pre_attn_rmsnorm.weight', 'model.layers.25.pre_moe_rmsnorm.weight', 'model.layers.26.self_attn.W_query.weight', 'model.layers.26.self_attn.W_key.weight', 'model.layers.26.self_attn.W_value.weight', 'model.layers.26.self_attn.W_output.weight', 'model.layers.26.moe.gate_bank', 'model.layers.26.moe.up_bank', 'model.layers.26.moe.down_bank', 'model.layers.26.moe.gate.weight', 'model.layers.26.pre_attn_rmsnorm.weight', 'model.layers.26.pre_moe_rmsnorm.weight', 'model.layers.27.self_attn.W_query.weight', 'model.layers.27.self_attn.W_key.weight', 'model.layers.27.self_attn.W_value.weight', 'model.layers.27.self_attn.W_output.weight', 'model.layers.27.moe.gate_bank', 'model.layers.27.moe.up_bank', 'model.layers.27.moe.down_bank', 'model.layers.27.moe.gate.weight', 'model.layers.27.pre_attn_rmsnorm.weight', 'model.layers.27.pre_moe_rmsnorm.weight', 'model.layers.28.self_attn.W_query.weight', 'model.layers.28.self_attn.W_key.weight', 'model.layers.28.self_attn.W_value.weight', 'model.layers.28.self_attn.W_output.weight', 'model.layers.28.moe.gate_bank', 'model.layers.28.moe.up_bank', 'model.layers.28.moe.down_bank', 'model.layers.28.moe.gate.weight', 'model.layers.28.pre_attn_rmsnorm.weight', 'model.layers.28.pre_moe_rmsnorm.weight', 'model.layers.29.self_attn.W_query.weight', 'model.layers.29.self_attn.W_key.weight', 'model.layers.29.self_attn.W_value.weight', 'model.layers.29.self_attn.W_output.weight', 'model.layers.29.moe.gate_bank', 'model.layers.29.moe.up_bank', 'model.layers.29.moe.down_bank', 'model.layers.29.moe.gate.weight', 'model.layers.29.pre_attn_rmsnorm.weight', 'model.layers.29.pre_moe_rmsnorm.weight', 'model.norm.weight', 'lm_head.weight'])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "util, lb_loss = model.get_expert_utilization()\n",
        "print(f\"Expert utilization per layer: {util.mean(dim=0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvaZrEgcIkMr",
        "outputId": "e4977757-a187-4be1-a8d6-0957bcf70171"
      },
      "id": "OvaZrEgcIkMr",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expert utilization per layer: tensor([0.2583, 0.6667, 0.0750])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(torch.argmax(outputs['logits'][0], dim=-1).tolist()))\n",
        "print(f\"Logits mean={outputs['logits'].mean().item()}, std={outputs['logits'].std().item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmjFtb4LIgkA",
        "outputId": "7170058d-64a6-4b41-d8e8-02007cbca349"
      },
      "id": "XmjFtb4LIgkA",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ation's.ation\n",
            "Logits mean=6.71024751663208, std=3.2853877544403076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ref_model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "ref_model.eval()\n",
        "input_ids = torch.tensor([[1, 2, 3, 4]])\n",
        "attention_mask = torch.ones(1, 4)\n",
        "\n",
        "with torch.no_grad():\n",
        "    try:\n",
        "        ref_outputs = ref_model(input_ids, attention_mask=attention_mask)\n",
        "        print(f\"Reference logits shape={ref_outputs.logits.shape}, mean={ref_outputs.logits.mean().item()}\")\n",
        "        # Access final hidden states\n",
        "        ref_hidden_states = ref_model.model(input_ids, attention_mask=attention_mask)[0]\n",
        "        ref_hidden_states = ref_model.model.norm(ref_hidden_states)\n",
        "        print(f\"Reference final hidden_states shape={ref_hidden_states.shape}, mean={ref_hidden_states.mean().item()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in reference model: {e}\")\n",
        "        # Fallback: Run only embedding and first layer\n",
        "        ref_hidden_states = ref_model.model.embed_tokens(input_ids)\n",
        "        print(f\"Reference embedding mean={ref_hidden_states.mean().item()}\")\n",
        "        ref_hidden_states = ref_model.model.layers[0](ref_hidden_states, attention_mask=attention_mask)[0]\n",
        "        print(f\"Reference layer 0 mean={ref_hidden_states.mean().item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sja80iD-ILHW",
        "outputId": "fd9e8ae3-d700-4104-bdab-7e9b7ce571a3"
      },
      "id": "Sja80iD-ILHW",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference logits shape=torch.Size([1, 4, 49152]), mean=4.597760200500488\n",
            "Reference final hidden_states shape=torch.Size([1, 4, 576]), mean=0.0934964045882225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "ref_model_config = AutoConfig.from_pretrained(checkpoint)\n",
        "print(ref_model_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eQBkc9HJV2I",
        "outputId": "07d33a0f-d0e1-491c-923d-32fea8b92b06"
      },
      "id": "3eQBkc9HJV2I",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"dtype\": \"bfloat16\",\n",
            "  \"eos_token_id\": 0,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 576,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 9,\n",
            "  \"num_hidden_layers\": 30,\n",
            "  \"num_key_value_heads\": 3,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 49152\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ref_model.modules)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5JKNuvrIbF9",
        "outputId": "1bc61bdc-6f2a-47a7-c4d1-aa5927051fee"
      },
      "id": "L5JKNuvrIbF9",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Module.modules of LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(49152, 576)\n",
            "    (layers): ModuleList(\n",
            "      (0-29): 30 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
            "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
            "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
            "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
            "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
            "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
            ")>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(300))\n",
        "print(tokenizer.decode(torch.argmax(outputs['logits'][0], dim=-1).tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YU-e2RYX9OG",
        "outputId": "037334ab-e4b0-4393-aa9c-be89ba225f5e"
      },
      "id": "0YU-e2RYX9OG",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "om\n",
            "ation's.ation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "util, lb_loss = model.get_expert_utilization()\n",
        "print(f\"Expert utilization per layer: {util.mean(dim=0)}\")  # Should show balanced use across experts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWD9rb9bcYUD",
        "outputId": "187dbac2-daa9-4423-aa1e-7ce67d60bb5d"
      },
      "id": "JWD9rb9bcYUD",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expert utilization per layer: tensor([0.2583, 0.6667, 0.0750])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "ref_model.eval()\n",
        "with torch.no_grad():\n",
        "    ref_outputs = ref_model(input_ids, attention_mask=attention_mask)\n",
        "print(ref_outputs.logits.shape, ref_outputs.logits.mean().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_wKb9QpckQ-",
        "outputId": "6c4c7f96-ce8a-404c-e9b4-b02eda98f5ef"
      },
      "id": "P_wKb9QpckQ-",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 49152]) 4.597760200500488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp2rnBAwnV3s",
        "outputId": "6fdc5974-58c8-4817-a018-3f0c0691bf37"
      },
      "id": "sp2rnBAwnV3s",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<issue_closed>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fa506c5-6066-4dc9-a252-586bd2c6ac48",
      "metadata": {
        "id": "1fa506c5-6066-4dc9-a252-586bd2c6ac48"
      },
      "source": [
        "### 3. Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "2a2c3f4d-55b7-4cbe-ab5c-daeded6785ca",
      "metadata": {
        "id": "2a2c3f4d-55b7-4cbe-ab5c-daeded6785ca"
      },
      "outputs": [],
      "source": [
        "######################################################################################################################\n",
        "############################################## DO NOT CHANGE[START] ##################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# Load the Tokenizer\n",
        "checkpoint=\"HuggingFaceTB/SmolLM-135M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "########################\n",
        "#### SANITY CHECK ######\n",
        "########################\n",
        "\n",
        "# # Instantiate the model\n",
        "# __test_model = smolMoELM(config)\n",
        "\n",
        "# #üí° You expect a nonsensical/garbled output here since the weights are random\n",
        "# generation_compare(\n",
        "#     prompt=TEST_PROMPT,\n",
        "#     tokenizer=tokenizer,\n",
        "#     num_tokens=50,\n",
        "#     model_A= __test_model,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "62a42a9a-df5c-48ac-ae18-65ad81bd5b10",
      "metadata": {
        "id": "62a42a9a-df5c-48ac-ae18-65ad81bd5b10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7702847a-67be-4b7d-bc16-2d2daca727be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>>>>>>>>>>>>>>>>>>\n",
            "\tPrompt\n",
            "<<<<<<<<<<<<<<<<<<<<\n",
            "Where is the Great Wall?\n",
            "\n",
            "\n",
            "time=9.840s\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\tModel_A Generation\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "\n",
            "\n",
            "The Great Wall of China is a 2,000-mile-long wall that spans 13,000 miles from the southern border of the Yellow River in Shandong province to the northern border of the ancestral\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "########################\n",
        "###### CHECK #1 ########\n",
        "########################\n",
        "\n",
        "# Instantiate the model\n",
        "__test_model = smolMoELM(config)\n",
        "\n",
        "# Load the weights into your \"fixed\" implementation\n",
        "__test_model.load_state_dict(torch.load('trial_weights.pt'), strict=True)\n",
        "\n",
        "\n",
        "#üí° If you fixed all bugs, you will see a sensible generation here :)\n",
        "generation_compare(\n",
        "    prompt=TEST_PROMPT,\n",
        "    tokenizer=tokenizer,\n",
        "    num_tokens=50,\n",
        "    model_A= __test_model,\n",
        "    model_B=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "228DepsRuYxF",
      "metadata": {
        "id": "228DepsRuYxF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc6bdefd-95d0-41d7-c6f8-1484a6ddc76e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Expected) Load Balance Loss => 1.00\n",
            "(Actual) Load Balance Loss => 1.00\n"
          ]
        }
      ],
      "source": [
        "########################\n",
        "###### CHECK #2 ########\n",
        "########################\n",
        "\n",
        "\n",
        "#üí° If you fixed all the bugs and completed the missing implementation you will match the load balancer loss that we precomputed\n",
        "correct_lb_loss = torch.tensor(1.0)\n",
        "_, lb_loss = __test_model.get_expert_utilization()\n",
        "print(f\"(Expected) Load Balance Loss => {correct_lb_loss:0.2f}\")\n",
        "print(f\"(Actual) Load Balance Loss => {lb_loss:0.2f}\")\n",
        "assert torch.isclose(lb_loss, correct_lb_loss, atol=1e-2), \"Load Balance Check don't match!\"\n",
        "\n",
        "######################################################################################################################\n",
        "############################################### DO NOT CHANGE[END] ###################################################\n",
        "######################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d97dfbd1-f738-4c1b-a7d7-d298e1404e48",
      "metadata": {
        "id": "d97dfbd1-f738-4c1b-a7d7-d298e1404e48"
      },
      "source": [
        "# **Coding Challenge Part 2: Upcycling a Dense Model into an MoE üîÑ üö¥ [üö® 3 points]**\n",
        "\n",
        "\n",
        "Now that we have worked through an implementation of the MoE architecture, lets look at a procedure called \"Upcycling\" wherein you convert a dense model into an MoE.\n",
        "\n",
        "**Guidelines** :\n",
        "\n",
        "You will upcycle the dense model loaded below into our MoE implementation from Part 1. No changes are required of the MoE implementation for this part.\n",
        "\n",
        "\n",
        "**üö® Reference paper:** [Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints](https://arxiv.org/abs/2212.05055)\n",
        "\n",
        "> *This paper introduces a method to transform pre-trained dense models into Mixture-of-Experts (MoE) models, leveraging existing weights instead of training from scratch. This \"upcycling\" approach selectively sparsifies the model into expert modules, enabling more efficient scaling and training while reducing computational costs. Experiments show that these upcycled MoEs can outperform both standard dense models and traditionally trained MoEs, demonstrating that dense checkpoints contain useful knowledge that can be repurposed for sparse architectures.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "888cc586-75ab-480a-bb72-97d3bca17710",
      "metadata": {
        "id": "888cc586-75ab-480a-bb72-97d3bca17710"
      },
      "source": [
        "### 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "a271079c-6113-4949-a057-0148625b6c4e",
      "metadata": {
        "id": "a271079c-6113-4949-a057-0148625b6c4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325f1d85-ebbf-42fd-99c8-9d23cd2780a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>>>>>>>>>>>>>>>>>>\n",
            "\tPrompt\n",
            "<<<<<<<<<<<<<<<<<<<<\n",
            "Where is the Great Wall?\n",
            "\n",
            "\n",
            "time=5.626s\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\tModel_A Generation\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "\n",
            "The Great Wall of China is the longest wall in the world. It stretches over 13,000 miles and is 13,000 feet high. It is located in the northern part of China, in the country\n",
            "\n",
            "\n",
            "\n",
            "time=9.599s\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\tModel_B Generation\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "amonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamonamon\n"
          ]
        }
      ],
      "source": [
        "######################################################################################################################\n",
        "############################################## DO NOT CHANGE[START] ##################################################\n",
        "######################################################################################################################\n",
        "\n",
        "\n",
        "# Loading the Dense Model\n",
        "dense_model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "\n",
        "# Resetting the weights for a clean upcycle!\n",
        "__test_model.reset_weights_and_metrics()\n",
        "\n",
        "#üí° This is expected to be garbled due to resetting weights before upcycling.\n",
        "generation_compare(\n",
        "    prompt=TEST_PROMPT,\n",
        "    tokenizer=tokenizer,\n",
        "    num_tokens=50,\n",
        "    model_A= dense_model,\n",
        "    model_B=__test_model\n",
        ")\n",
        "\n",
        "######################################################################################################################\n",
        "############################################### DO NOT CHANGE[END] ###################################################\n",
        "######################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f997d0c9-2d88-4331-9b64-daa35cf106f0",
      "metadata": {
        "id": "f997d0c9-2d88-4331-9b64-daa35cf106f0",
        "outputId": "dfa5f367-7214-4506-b60a-064d1ea7f6b7",
        "scrolled": true
      },
      "source": [
        "### 2. Upcycling (for Implementation)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict().keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USymWcUIvXlC",
        "outputId": "1f9d3f98-542e-4637-a364-a03f77ec014b"
      },
      "id": "USymWcUIvXlC",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['model.embed_tokens.weight', 'model.layers.0.self_attn.W_query.weight', 'model.layers.0.self_attn.W_key.weight', 'model.layers.0.self_attn.W_value.weight', 'model.layers.0.self_attn.W_output.weight', 'model.layers.0.moe.gate_bank', 'model.layers.0.moe.up_bank', 'model.layers.0.moe.down_bank', 'model.layers.0.moe.gate.weight', 'model.layers.0.pre_attn_rmsnorm.weight', 'model.layers.0.pre_moe_rmsnorm.weight', 'model.layers.1.self_attn.W_query.weight', 'model.layers.1.self_attn.W_key.weight', 'model.layers.1.self_attn.W_value.weight', 'model.layers.1.self_attn.W_output.weight', 'model.layers.1.moe.gate_bank', 'model.layers.1.moe.up_bank', 'model.layers.1.moe.down_bank', 'model.layers.1.moe.gate.weight', 'model.layers.1.pre_attn_rmsnorm.weight', 'model.layers.1.pre_moe_rmsnorm.weight', 'model.layers.2.self_attn.W_query.weight', 'model.layers.2.self_attn.W_key.weight', 'model.layers.2.self_attn.W_value.weight', 'model.layers.2.self_attn.W_output.weight', 'model.layers.2.moe.gate_bank', 'model.layers.2.moe.up_bank', 'model.layers.2.moe.down_bank', 'model.layers.2.moe.gate.weight', 'model.layers.2.pre_attn_rmsnorm.weight', 'model.layers.2.pre_moe_rmsnorm.weight', 'model.layers.3.self_attn.W_query.weight', 'model.layers.3.self_attn.W_key.weight', 'model.layers.3.self_attn.W_value.weight', 'model.layers.3.self_attn.W_output.weight', 'model.layers.3.moe.gate_bank', 'model.layers.3.moe.up_bank', 'model.layers.3.moe.down_bank', 'model.layers.3.moe.gate.weight', 'model.layers.3.pre_attn_rmsnorm.weight', 'model.layers.3.pre_moe_rmsnorm.weight', 'model.layers.4.self_attn.W_query.weight', 'model.layers.4.self_attn.W_key.weight', 'model.layers.4.self_attn.W_value.weight', 'model.layers.4.self_attn.W_output.weight', 'model.layers.4.moe.gate_bank', 'model.layers.4.moe.up_bank', 'model.layers.4.moe.down_bank', 'model.layers.4.moe.gate.weight', 'model.layers.4.pre_attn_rmsnorm.weight', 'model.layers.4.pre_moe_rmsnorm.weight', 'model.layers.5.self_attn.W_query.weight', 'model.layers.5.self_attn.W_key.weight', 'model.layers.5.self_attn.W_value.weight', 'model.layers.5.self_attn.W_output.weight', 'model.layers.5.moe.gate_bank', 'model.layers.5.moe.up_bank', 'model.layers.5.moe.down_bank', 'model.layers.5.moe.gate.weight', 'model.layers.5.pre_attn_rmsnorm.weight', 'model.layers.5.pre_moe_rmsnorm.weight', 'model.layers.6.self_attn.W_query.weight', 'model.layers.6.self_attn.W_key.weight', 'model.layers.6.self_attn.W_value.weight', 'model.layers.6.self_attn.W_output.weight', 'model.layers.6.moe.gate_bank', 'model.layers.6.moe.up_bank', 'model.layers.6.moe.down_bank', 'model.layers.6.moe.gate.weight', 'model.layers.6.pre_attn_rmsnorm.weight', 'model.layers.6.pre_moe_rmsnorm.weight', 'model.layers.7.self_attn.W_query.weight', 'model.layers.7.self_attn.W_key.weight', 'model.layers.7.self_attn.W_value.weight', 'model.layers.7.self_attn.W_output.weight', 'model.layers.7.moe.gate_bank', 'model.layers.7.moe.up_bank', 'model.layers.7.moe.down_bank', 'model.layers.7.moe.gate.weight', 'model.layers.7.pre_attn_rmsnorm.weight', 'model.layers.7.pre_moe_rmsnorm.weight', 'model.layers.8.self_attn.W_query.weight', 'model.layers.8.self_attn.W_key.weight', 'model.layers.8.self_attn.W_value.weight', 'model.layers.8.self_attn.W_output.weight', 'model.layers.8.moe.gate_bank', 'model.layers.8.moe.up_bank', 'model.layers.8.moe.down_bank', 'model.layers.8.moe.gate.weight', 'model.layers.8.pre_attn_rmsnorm.weight', 'model.layers.8.pre_moe_rmsnorm.weight', 'model.layers.9.self_attn.W_query.weight', 'model.layers.9.self_attn.W_key.weight', 'model.layers.9.self_attn.W_value.weight', 'model.layers.9.self_attn.W_output.weight', 'model.layers.9.moe.gate_bank', 'model.layers.9.moe.up_bank', 'model.layers.9.moe.down_bank', 'model.layers.9.moe.gate.weight', 'model.layers.9.pre_attn_rmsnorm.weight', 'model.layers.9.pre_moe_rmsnorm.weight', 'model.layers.10.self_attn.W_query.weight', 'model.layers.10.self_attn.W_key.weight', 'model.layers.10.self_attn.W_value.weight', 'model.layers.10.self_attn.W_output.weight', 'model.layers.10.moe.gate_bank', 'model.layers.10.moe.up_bank', 'model.layers.10.moe.down_bank', 'model.layers.10.moe.gate.weight', 'model.layers.10.pre_attn_rmsnorm.weight', 'model.layers.10.pre_moe_rmsnorm.weight', 'model.layers.11.self_attn.W_query.weight', 'model.layers.11.self_attn.W_key.weight', 'model.layers.11.self_attn.W_value.weight', 'model.layers.11.self_attn.W_output.weight', 'model.layers.11.moe.gate_bank', 'model.layers.11.moe.up_bank', 'model.layers.11.moe.down_bank', 'model.layers.11.moe.gate.weight', 'model.layers.11.pre_attn_rmsnorm.weight', 'model.layers.11.pre_moe_rmsnorm.weight', 'model.layers.12.self_attn.W_query.weight', 'model.layers.12.self_attn.W_key.weight', 'model.layers.12.self_attn.W_value.weight', 'model.layers.12.self_attn.W_output.weight', 'model.layers.12.moe.gate_bank', 'model.layers.12.moe.up_bank', 'model.layers.12.moe.down_bank', 'model.layers.12.moe.gate.weight', 'model.layers.12.pre_attn_rmsnorm.weight', 'model.layers.12.pre_moe_rmsnorm.weight', 'model.layers.13.self_attn.W_query.weight', 'model.layers.13.self_attn.W_key.weight', 'model.layers.13.self_attn.W_value.weight', 'model.layers.13.self_attn.W_output.weight', 'model.layers.13.moe.gate_bank', 'model.layers.13.moe.up_bank', 'model.layers.13.moe.down_bank', 'model.layers.13.moe.gate.weight', 'model.layers.13.pre_attn_rmsnorm.weight', 'model.layers.13.pre_moe_rmsnorm.weight', 'model.layers.14.self_attn.W_query.weight', 'model.layers.14.self_attn.W_key.weight', 'model.layers.14.self_attn.W_value.weight', 'model.layers.14.self_attn.W_output.weight', 'model.layers.14.moe.gate_bank', 'model.layers.14.moe.up_bank', 'model.layers.14.moe.down_bank', 'model.layers.14.moe.gate.weight', 'model.layers.14.pre_attn_rmsnorm.weight', 'model.layers.14.pre_moe_rmsnorm.weight', 'model.layers.15.self_attn.W_query.weight', 'model.layers.15.self_attn.W_key.weight', 'model.layers.15.self_attn.W_value.weight', 'model.layers.15.self_attn.W_output.weight', 'model.layers.15.moe.gate_bank', 'model.layers.15.moe.up_bank', 'model.layers.15.moe.down_bank', 'model.layers.15.moe.gate.weight', 'model.layers.15.pre_attn_rmsnorm.weight', 'model.layers.15.pre_moe_rmsnorm.weight', 'model.layers.16.self_attn.W_query.weight', 'model.layers.16.self_attn.W_key.weight', 'model.layers.16.self_attn.W_value.weight', 'model.layers.16.self_attn.W_output.weight', 'model.layers.16.moe.gate_bank', 'model.layers.16.moe.up_bank', 'model.layers.16.moe.down_bank', 'model.layers.16.moe.gate.weight', 'model.layers.16.pre_attn_rmsnorm.weight', 'model.layers.16.pre_moe_rmsnorm.weight', 'model.layers.17.self_attn.W_query.weight', 'model.layers.17.self_attn.W_key.weight', 'model.layers.17.self_attn.W_value.weight', 'model.layers.17.self_attn.W_output.weight', 'model.layers.17.moe.gate_bank', 'model.layers.17.moe.up_bank', 'model.layers.17.moe.down_bank', 'model.layers.17.moe.gate.weight', 'model.layers.17.pre_attn_rmsnorm.weight', 'model.layers.17.pre_moe_rmsnorm.weight', 'model.layers.18.self_attn.W_query.weight', 'model.layers.18.self_attn.W_key.weight', 'model.layers.18.self_attn.W_value.weight', 'model.layers.18.self_attn.W_output.weight', 'model.layers.18.moe.gate_bank', 'model.layers.18.moe.up_bank', 'model.layers.18.moe.down_bank', 'model.layers.18.moe.gate.weight', 'model.layers.18.pre_attn_rmsnorm.weight', 'model.layers.18.pre_moe_rmsnorm.weight', 'model.layers.19.self_attn.W_query.weight', 'model.layers.19.self_attn.W_key.weight', 'model.layers.19.self_attn.W_value.weight', 'model.layers.19.self_attn.W_output.weight', 'model.layers.19.moe.gate_bank', 'model.layers.19.moe.up_bank', 'model.layers.19.moe.down_bank', 'model.layers.19.moe.gate.weight', 'model.layers.19.pre_attn_rmsnorm.weight', 'model.layers.19.pre_moe_rmsnorm.weight', 'model.layers.20.self_attn.W_query.weight', 'model.layers.20.self_attn.W_key.weight', 'model.layers.20.self_attn.W_value.weight', 'model.layers.20.self_attn.W_output.weight', 'model.layers.20.moe.gate_bank', 'model.layers.20.moe.up_bank', 'model.layers.20.moe.down_bank', 'model.layers.20.moe.gate.weight', 'model.layers.20.pre_attn_rmsnorm.weight', 'model.layers.20.pre_moe_rmsnorm.weight', 'model.layers.21.self_attn.W_query.weight', 'model.layers.21.self_attn.W_key.weight', 'model.layers.21.self_attn.W_value.weight', 'model.layers.21.self_attn.W_output.weight', 'model.layers.21.moe.gate_bank', 'model.layers.21.moe.up_bank', 'model.layers.21.moe.down_bank', 'model.layers.21.moe.gate.weight', 'model.layers.21.pre_attn_rmsnorm.weight', 'model.layers.21.pre_moe_rmsnorm.weight', 'model.layers.22.self_attn.W_query.weight', 'model.layers.22.self_attn.W_key.weight', 'model.layers.22.self_attn.W_value.weight', 'model.layers.22.self_attn.W_output.weight', 'model.layers.22.moe.gate_bank', 'model.layers.22.moe.up_bank', 'model.layers.22.moe.down_bank', 'model.layers.22.moe.gate.weight', 'model.layers.22.pre_attn_rmsnorm.weight', 'model.layers.22.pre_moe_rmsnorm.weight', 'model.layers.23.self_attn.W_query.weight', 'model.layers.23.self_attn.W_key.weight', 'model.layers.23.self_attn.W_value.weight', 'model.layers.23.self_attn.W_output.weight', 'model.layers.23.moe.gate_bank', 'model.layers.23.moe.up_bank', 'model.layers.23.moe.down_bank', 'model.layers.23.moe.gate.weight', 'model.layers.23.pre_attn_rmsnorm.weight', 'model.layers.23.pre_moe_rmsnorm.weight', 'model.layers.24.self_attn.W_query.weight', 'model.layers.24.self_attn.W_key.weight', 'model.layers.24.self_attn.W_value.weight', 'model.layers.24.self_attn.W_output.weight', 'model.layers.24.moe.gate_bank', 'model.layers.24.moe.up_bank', 'model.layers.24.moe.down_bank', 'model.layers.24.moe.gate.weight', 'model.layers.24.pre_attn_rmsnorm.weight', 'model.layers.24.pre_moe_rmsnorm.weight', 'model.layers.25.self_attn.W_query.weight', 'model.layers.25.self_attn.W_key.weight', 'model.layers.25.self_attn.W_value.weight', 'model.layers.25.self_attn.W_output.weight', 'model.layers.25.moe.gate_bank', 'model.layers.25.moe.up_bank', 'model.layers.25.moe.down_bank', 'model.layers.25.moe.gate.weight', 'model.layers.25.pre_attn_rmsnorm.weight', 'model.layers.25.pre_moe_rmsnorm.weight', 'model.layers.26.self_attn.W_query.weight', 'model.layers.26.self_attn.W_key.weight', 'model.layers.26.self_attn.W_value.weight', 'model.layers.26.self_attn.W_output.weight', 'model.layers.26.moe.gate_bank', 'model.layers.26.moe.up_bank', 'model.layers.26.moe.down_bank', 'model.layers.26.moe.gate.weight', 'model.layers.26.pre_attn_rmsnorm.weight', 'model.layers.26.pre_moe_rmsnorm.weight', 'model.layers.27.self_attn.W_query.weight', 'model.layers.27.self_attn.W_key.weight', 'model.layers.27.self_attn.W_value.weight', 'model.layers.27.self_attn.W_output.weight', 'model.layers.27.moe.gate_bank', 'model.layers.27.moe.up_bank', 'model.layers.27.moe.down_bank', 'model.layers.27.moe.gate.weight', 'model.layers.27.pre_attn_rmsnorm.weight', 'model.layers.27.pre_moe_rmsnorm.weight', 'model.layers.28.self_attn.W_query.weight', 'model.layers.28.self_attn.W_key.weight', 'model.layers.28.self_attn.W_value.weight', 'model.layers.28.self_attn.W_output.weight', 'model.layers.28.moe.gate_bank', 'model.layers.28.moe.up_bank', 'model.layers.28.moe.down_bank', 'model.layers.28.moe.gate.weight', 'model.layers.28.pre_attn_rmsnorm.weight', 'model.layers.28.pre_moe_rmsnorm.weight', 'model.layers.29.self_attn.W_query.weight', 'model.layers.29.self_attn.W_key.weight', 'model.layers.29.self_attn.W_value.weight', 'model.layers.29.self_attn.W_output.weight', 'model.layers.29.moe.gate_bank', 'model.layers.29.moe.up_bank', 'model.layers.29.moe.down_bank', 'model.layers.29.moe.gate.weight', 'model.layers.29.pre_attn_rmsnorm.weight', 'model.layers.29.pre_moe_rmsnorm.weight', 'model.norm.weight', 'lm_head.weight'])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "e95894a3-53eb-43ad-b1a5-eb19259191ca",
      "metadata": {
        "id": "e95894a3-53eb-43ad-b1a5-eb19259191ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a93b2509-f995-447a-bc0f-66ca33e1022c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting the upcycling process...\n",
            "Upcycling layer 1/30...\n",
            "Upcycling layer 2/30...\n",
            "Upcycling layer 3/30...\n",
            "Upcycling layer 4/30...\n",
            "Upcycling layer 5/30...\n",
            "Upcycling layer 6/30...\n",
            "Upcycling layer 7/30...\n",
            "Upcycling layer 8/30...\n",
            "Upcycling layer 9/30...\n",
            "Upcycling layer 10/30...\n",
            "Upcycling layer 11/30...\n",
            "Upcycling layer 12/30...\n",
            "Upcycling layer 13/30...\n",
            "Upcycling layer 14/30...\n",
            "Upcycling layer 15/30...\n",
            "Upcycling layer 16/30...\n",
            "Upcycling layer 17/30...\n",
            "Upcycling layer 18/30...\n",
            "Upcycling layer 19/30...\n",
            "Upcycling layer 20/30...\n",
            "Upcycling layer 21/30...\n",
            "Upcycling layer 22/30...\n",
            "Upcycling layer 23/30...\n",
            "Upcycling layer 24/30...\n",
            "Upcycling layer 25/30...\n",
            "Upcycling layer 26/30...\n",
            "Upcycling layer 27/30...\n",
            "Upcycling layer 28/30...\n",
            "Upcycling layer 29/30...\n",
            "Upcycling layer 30/30...\n",
            "\n",
            "Upcycling complete!\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "##################### Write \"Upcycling\" code here ####################\n",
        "######################################################################\n",
        "\n",
        "\n",
        "dense_sd = dense_model.state_dict()\n",
        "moe_sd = __test_model.state_dict()\n",
        "\n",
        "print(\"Starting the upcycling process...\")\n",
        "\n",
        "# 1. Copy shared weights (Embeddings and Final Norm)\n",
        "# These names are usually consistent\n",
        "moe_sd['model.embed_tokens.weight'] = dense_sd['model.embed_tokens.weight']\n",
        "moe_sd['model.norm.weight'] = dense_sd['model.norm.weight']\n",
        "moe_sd['lm_head.weight'] = dense_sd['lm_head.weight']\n",
        "\n",
        "\n",
        "# 2. Define the name mappings for attention and MLP layers\n",
        "attention_mapping = {\n",
        "    \"q_proj\": \"W_query\",\n",
        "    \"k_proj\": \"W_key\",\n",
        "    \"v_proj\": \"W_value\",\n",
        "    \"o_proj\": \"W_output\",\n",
        "}\n",
        "\n",
        "# 3. Iterate through each layer to copy weights\n",
        "for i in range(config.num_hidden_layers):\n",
        "    print(f\"Upcycling layer {i+1}/{config.num_hidden_layers}...\")\n",
        "\n",
        "    # --- Copy Self-Attention weights ---\n",
        "    for dense_name, moe_name in attention_mapping.items():\n",
        "        dense_key = f'model.layers.{i}.self_attn.{dense_name}.weight'\n",
        "        moe_key = f'model.layers.{i}.self_attn.{moe_name}.weight'\n",
        "        moe_sd[moe_key] = dense_sd[dense_key]\n",
        "\n",
        "    # --- Copy LayerNorm weights ---\n",
        "    moe_sd[f'model.layers.{i}.pre_attn_rmsnorm.weight'] = dense_sd[f'model.layers.{i}.input_layernorm.weight']\n",
        "    moe_sd[f'model.layers.{i}.pre_moe_rmsnorm.weight'] = dense_sd[f'model.layers.{i}.post_attention_layernorm.weight']\n",
        "\n",
        "    # --- Upcycle the dense MLP to the MoE expert banks ---\n",
        "\n",
        "    # Correctly map and TRANSPOSE the MLP weights\n",
        "    gate_weight = dense_sd[f'model.layers.{i}.mlp.gate_proj.weight'].transpose(-2, -1)\n",
        "    up_weight = dense_sd[f'model.layers.{i}.mlp.up_proj.weight'].transpose(-2, -1)\n",
        "    down_weight = dense_sd[f'model.layers.{i}.mlp.down_proj.weight'].transpose(-2, -1)\n",
        "\n",
        "    moe_sd[f'model.layers.{i}.moe.gate_bank'] = gate_weight.unsqueeze(0).expand(config.num_experts, -1, -1)\n",
        "    moe_sd[f'model.layers.{i}.moe.up_bank'] = up_weight.unsqueeze(0).expand(config.num_experts, -1, -1)\n",
        "    moe_sd[f'model.layers.{i}.moe.down_bank'] = down_weight.unsqueeze(0).expand(config.num_experts, -1, -1)\n",
        "\n",
        "    # --- Initialize the MoE router gate weights to zero ---\n",
        "    nn.init.zeros_(moe_sd[f'model.layers.{i}.moe.gate.weight'])\n",
        "\n",
        "\n",
        "# Load the newly created state dictionary into our MoE model\n",
        "__test_model.load_state_dict(moe_sd)\n",
        "\n",
        "print(\"\\nUpcycling complete!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60d79a9c-5b32-4a4d-a4b4-97d71b82154d",
      "metadata": {
        "id": "60d79a9c-5b32-4a4d-a4b4-97d71b82154d"
      },
      "source": [
        "### 3. Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "cd3c638d-70b8-4016-8633-78cc45dd93ff",
      "metadata": {
        "id": "cd3c638d-70b8-4016-8633-78cc45dd93ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17eae706-041d-4586-92df-beb6d20bf1b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>>>>>>>>>>>>>>>>>>\n",
            "\tPrompt\n",
            "<<<<<<<<<<<<<<<<<<<<\n",
            "Where is the Great Wall?\n",
            "\n",
            "\n",
            "time=5.754s\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\tModel_A Generation\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "\n",
            "The Great Wall of China is the longest wall in the world. It stretches over 13,000 miles and is 13,000 feet high. It is located in the northern part of China, in the country\n",
            "\n",
            "\n",
            "\n",
            "time=9.340s\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\tModel_B Generation\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "\n",
            "The Great Wall of China is the longest wall in the world. It stretches over 13,000 miles and is 13,000 feet high. It is located in the northern part of China, in the country\n"
          ]
        }
      ],
      "source": [
        "######################################################################################################################\n",
        "############################################## DO NOT CHANGE[START] ##################################################\n",
        "######################################################################################################################\n",
        "\n",
        "\n",
        "#üí° If you upcycled correctly, you will output the exact same generation as the dense model!\n",
        "generation_compare(\n",
        "    prompt=TEST_PROMPT,\n",
        "    tokenizer=tokenizer,\n",
        "    num_tokens=50,\n",
        "    model_A= dense_model,\n",
        "    model_B=__test_model\n",
        ")\n",
        "\n",
        "\n",
        "######################################################################################################################\n",
        "############################################### DO NOT CHANGE[END] ###################################################\n",
        "######################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06630c02-567c-4c60-bca8-ac736030ca6b",
      "metadata": {
        "id": "06630c02-567c-4c60-bca8-ac736030ca6b"
      },
      "source": [
        "# **Coding Challenge Part 3: Continued Pretraining üìöüí™ [üö® 7 points]**\n",
        "\n",
        "**Note** :\n",
        "*   For this section, make sure that the model you are using is still the same `__test_model` you upcycled in the previous section.\n",
        "*   We recommend using a GPU for this section. We have provided the below settings and ensure that they run on the free T4 GPUs on Colab. Make sure you manage your free GPU usage wisely :)\n",
        "\n",
        "\n",
        "Now that we have an upcycled MoE, lets continue pretraining on a small subset of data to train the expert router.\n",
        "\n",
        "You will be required to :\n",
        "* 1. Write a simple training loop (*and implement functions related to this*)\n",
        "* 2. **Propose a MoE-specific metric** to track whether the MoE is actually learning as expected, implement it and provide a 2 line description of your metric in the space provided."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "773e3941-6cf7-47af-bfd8-6c1a7dbfe332",
      "metadata": {
        "id": "773e3941-6cf7-47af-bfd8-6c1a7dbfe332"
      },
      "source": [
        "### 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92599fa7-b695-45d6-8501-ac64b0bdfced",
      "metadata": {
        "id": "92599fa7-b695-45d6-8501-ac64b0bdfced"
      },
      "outputs": [],
      "source": [
        "######################################################################################################################\n",
        "############################################## DO NOT CHANGE[START] ##################################################\n",
        "######################################################################################################################\n",
        "\n",
        "STEPS = 100\n",
        "REPORT_AFTER_N_STEPS = 10\n",
        "BATCH_SIZE = 4\n",
        "BF16 = True\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "__test_model.to(device) ### Note: This should be the upcycled model as a result of completing Part 2.\n",
        "print(f\"Using Device : {device}\")\n",
        "\n",
        "scaler_enabled = (device==\"cuda\" and BF16)\n",
        "autocast_dtype = torch.bfloat16 if scaler_enabled else None\n",
        "\n",
        "def build_dataset(\n",
        "    dataset_id,\n",
        "    subset,\n",
        "    split,\n",
        "    tokenizer,\n",
        "    block_size,\n",
        "    max_samples=1000,\n",
        "    text_column=\"text\",\n",
        "    val_fraction=None,\n",
        "    seed=42,\n",
        "):\n",
        "    ds = load_dataset(dataset_id, subset, split=split) if subset else load_dataset(dataset_id, split=split)\n",
        "    ds = ds.select(range(max_samples))\n",
        "\n",
        "    EOS = tokenizer.eos_token_id\n",
        "    def tok(batch):\n",
        "        out = tokenizer(batch[text_column],\n",
        "                        add_special_tokens=False,\n",
        "                        return_attention_mask=True)\n",
        "        out[\"input_ids\"]      = [ids + [EOS] for ids in out[\"input_ids\"]]\n",
        "        out[\"attention_mask\"] = [m   + [1]   for m   in out[\"attention_mask\"]]\n",
        "        return {\"input_ids\": out[\"input_ids\"], \"attention_mask\": out[\"attention_mask\"]}\n",
        "\n",
        "    ds = ds.map(tok, batched=True,remove_columns=[c for c in ds.column_names if c not in (\"input_ids\", \"attention_mask\")])\n",
        "\n",
        "    def group_per_doc(batch):\n",
        "        out_ids = []\n",
        "        for ids in batch[\"input_ids\"]:\n",
        "            L = len(ids)\n",
        "            n = (L // block_size) * block_size\n",
        "            for i in range(0, n, block_size):\n",
        "                out_ids.append(ids[i:i+block_size])\n",
        "        return {\"input_ids\": out_ids, \"attention_mask\": [[1]*len(o) for o in out_ids]}\n",
        "\n",
        "    ds = ds.map(group_per_doc, batched=True)\n",
        "\n",
        "    if val_fraction and 0.0 < val_fraction < 1.0:\n",
        "        ds = ds.train_test_split(test_size=val_fraction, seed=seed, shuffle=True)\n",
        "        train_ds, val_ds = ds[\"train\"], ds[\"test\"]\n",
        "        train_ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\"])\n",
        "        val_ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\"])\n",
        "        return train_ds, val_ds\n",
        "\n",
        "    ds.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\"])\n",
        "    return ds\n",
        "\n",
        "\n",
        "train_ds, val_ds = build_dataset(dataset_id=\"HuggingFaceTB/cosmopedia-100k\",\n",
        "                                 subset=None,\n",
        "                                 split=\"train\",\n",
        "                                 tokenizer=tokenizer,\n",
        "                                 block_size=256, # This is intentionally small number DO NOT change this number.\n",
        "                                 val_fraction=0.2,   # 20% as validation\n",
        "                                 max_samples=1000, # This only picks first 1000 examples from the dataset. Do NOT change this number.\n",
        "                                 seed=789)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=(device==\"cuda\")\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False, pin_memory=(device==\"cuda\")\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Train Dataset Batches : {len(train_loader)}\")\n",
        "print(f\"Validation Dataset Batches : {len(val_loader)}\")\n",
        "\n",
        "######################################################################################################################\n",
        "############################################### DO NOT CHANGE[END] ###################################################\n",
        "######################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b25d86bc-c981-4619-af9a-b7896b0516c5",
      "metadata": {
        "id": "b25d86bc-c981-4619-af9a-b7896b0516c5"
      },
      "source": [
        "### 2. Continued Pretraining (for Implementation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eecf407-f573-4d54-83f0-529a63bd7427",
      "metadata": {
        "id": "2eecf407-f573-4d54-83f0-529a63bd7427"
      },
      "outputs": [],
      "source": [
        "############################################\n",
        "############# Training Settings ############\n",
        "############################################\n",
        "LEARNING_RATE = None\n",
        "\n",
        "\n",
        "############################################\n",
        "############## OPTIMIZER ###################\n",
        "############################################\n",
        "\n",
        "opt = None\n",
        "scheduler = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "to4OGfvBfwLo",
      "metadata": {
        "id": "to4OGfvBfwLo"
      },
      "source": [
        "#### Helper Functions(üö® 4 points) :\n",
        "* *causal_lm_loss*(üö® 1 point)\n",
        "*  *eval_loss* (üö® 1 point)\n",
        "*  *custom_moe_metric* (üö® 2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc1f8eea-f0e0-4750-9ecb-6a38b363d3d2",
      "metadata": {
        "id": "fc1f8eea-f0e0-4750-9ecb-6a38b363d3d2"
      },
      "outputs": [],
      "source": [
        "############################################\n",
        "############## HELPER FNS ##################\n",
        "############################################\n",
        "\n",
        "def causal_lm_loss(...):\n",
        "    # Implement this\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_loss(...):\n",
        "    # Implement this\n",
        "\n",
        "@labelthis('Name This Metric')\n",
        "@torch.no_grad()\n",
        "def custom_moe_metric(...):\n",
        "    # Implement this\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LtGfE8qggfJ-",
      "metadata": {
        "id": "LtGfE8qggfJ-"
      },
      "source": [
        "####  Training loop (üö® 2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1941edb7-e533-4a38-9ff5-e12d39b203be",
      "metadata": {
        "id": "1941edb7-e533-4a38-9ff5-e12d39b203be"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "############### Write \"Continued Pretraining\" code here ##############\n",
        "######################################################################\n",
        "\n",
        "moe_metric = custom_moe_metric(__test_model)\n",
        "print(f\"[Before Training : Sanity Check] {custom_moe_metric.label}: {moe_metric:.1f}%\\n\")\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "loss = None\n",
        "\n",
        "training_metrics = {'Train Loss': [], 'Eval Loss': [], 'Load Balancing Loss': []}\n",
        "moe_metrics = {custom_moe_metric.label: []}\n",
        "\n",
        "for step in range(1, STEPS+1):\n",
        "\n",
        "   #########################################################\n",
        "   ############## Eval/Reporting Section ###################\n",
        "   #########################################################\n",
        "    if step % REPORT_AFTER_N_STEPS == 0:\n",
        "        val_loss = ...\n",
        "\n",
        "        training_metrics['Train Loss'].append()\n",
        "        training_metrics['Eval Loss'].append()\n",
        "        training_metrics['Load Balancing Loss'].append()\n",
        "\n",
        "        moe_metric = custom_moe_metric(__test_model)\n",
        "        metrics[custom_moe_metric.label].append(moe_metric)\n",
        "\n",
        "        time_taken= (time.time()-t0)\n",
        "        # KEEP THE SAME FORMATTING\n",
        "        print(f\"Step {step}/{STEPS} | Train Loss: {...:.3f} | Eval Loss: {...:.3f} | LB Loss: {...:.3f} | Time Taken: {pretty_dt(time_taken)}\")\n",
        "        print(\"***\"*30)\n",
        "        t0 = time.time()\n",
        "\n",
        "   ###################################################\n",
        "   ############## Training Section ###################\n",
        "   ###################################################\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0a94445-96d5-4a79-8f21-86e549abe068",
      "metadata": {
        "id": "a0a94445-96d5-4a79-8f21-86e549abe068"
      },
      "source": [
        "### 3. Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfae7fc3-7c7d-4c13-9987-68b26cb10ea0",
      "metadata": {
        "id": "dfae7fc3-7c7d-4c13-9987-68b26cb10ea0"
      },
      "outputs": [],
      "source": [
        "######################################################################################################################\n",
        "############################################## DO NOT CHANGE[START] ##################################################\n",
        "######################################################################################################################\n",
        "\n",
        "# Verify plots\n",
        "x_vals = [REPORT_AFTER_N_STEPS * i for i in range(1, len(training_metrics['Train Loss'])+1)]\n",
        "plot_metrics(training_metrics, x_vals=x_vals, suptitle=\"Training Metrics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "264d04f5-1671-4b96-acb2-7039d9ee9352",
      "metadata": {
        "id": "264d04f5-1671-4b96-acb2-7039d9ee9352"
      },
      "source": [
        "#### Plot MoE Metric with Explanation(üö® 1 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HLXIExE7kLy5",
      "metadata": {
        "id": "HLXIExE7kLy5"
      },
      "outputs": [],
      "source": [
        "######################################################################################################################\n",
        "############################################## PLOT YOUR CUSTOM MOE METRICS ##################################################\n",
        "######################################################################################################################\n",
        "\n",
        "def plot_custom_metric(metrics: dict, suptitle=None):\n",
        "    fig.suptitle(suptitle)\n",
        "    plt.show()\n",
        "\n",
        "plot_custom_metric(moe_metrics, suptitle=\"SOME TITLE HERE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8000336a-40d5-4f89-930b-aec804121c9a",
      "metadata": {
        "id": "8000336a-40d5-4f89-930b-aec804121c9a"
      },
      "source": [
        "### Why I chose `CUSTOM MOE METRIC`\n",
        "...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "413a3c92-51f7-429b-aef7-bfafc8740194",
      "metadata": {
        "id": "413a3c92-51f7-429b-aef7-bfafc8740194"
      },
      "outputs": [],
      "source": [
        "#üí° Verify that the model didn't collapse and can still generate coherent text.\n",
        "#   You dont expect this to be the same as the dense model, but should still be coherent\n",
        "__test_model.to('cpu')\n",
        "__test_model.eval()\n",
        "\n",
        "generation_compare(\n",
        "    prompt=TEST_PROMPT,\n",
        "    tokenizer=tokenizer,\n",
        "    num_tokens=50,\n",
        "    model_A= dense_model,\n",
        "    model_B=__test_model\n",
        ")\n",
        "\n",
        "######################################################################################################################\n",
        "############################################### DO NOT CHANGE[END] ###################################################\n",
        "######################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61950d09-19a3-4a37-9c0e-2df53557505c",
      "metadata": {
        "id": "61950d09-19a3-4a37-9c0e-2df53557505c"
      },
      "source": [
        "# **Coding Challenge Part 4:  Exploring The Unknown üßô ‚ú® [üö® 5 points]**\n",
        "\n",
        "In this part, you can choose any one of the provided questions below.\n",
        "\n",
        "Both questions are open-ended, and there is no one single solution -- you can follow any paper you find related to the question you picked and also you can be fully creative.\n",
        "\n",
        "We want to see how you will approach the problem and how you will show that your approach is working.    \n",
        "\n",
        "1. **Make training more efficient with dataset intervention:** Now you can process the whole dataset ([cosmopedia-100k](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia-100k), but you can only sample the same number of examples (1000). How would you modify/filter the original dataset for making the training more efficient?\n",
        "\n",
        "2. **Explore methods to increase expert specilization for given datasets:** You are given these 3 datasets inside [Nemotron-Post-Training-Dataset (SFT partition)](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset): [chat, math and code subsets](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset#filtering-the-data); develop training methods/pipelines that increase expert specialization for each data. (Each expert will focus on one of these datasets rathen than distributing uniformly.)\n",
        "\n",
        "**NOTE:** If your MoE implementation does not work, you can pick the 1 question and show the effectiness of your method on dense model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74b87b98-951e-4f79-8ec1-2b34c8dda636",
      "metadata": {
        "id": "74b87b98-951e-4f79-8ec1-2b34c8dda636"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "679385ad459740cc8fd85deaa3a01770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5bf759f6959146af9e007269590b32cc",
              "IPY_MODEL_6c57e5e68f074c32aa5d694d780b54c8",
              "IPY_MODEL_99ceee44faf4434fa23fd46e22d4b003"
            ],
            "layout": "IPY_MODEL_07e854d90ad94c5e85ab7efc6e8a36e6"
          }
        },
        "5bf759f6959146af9e007269590b32cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65df1c9dddbf4665b43d477f26c17cf4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6b829aeec9314d95b3de85cf23fc77f2",
            "value": "trial_weights.pt:‚Äá100%"
          }
        },
        "6c57e5e68f074c32aa5d694d780b54c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_350d036fe5624c08966d85824e602718",
            "max": 1175378467,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61849b775da848e9ae172c1f12c13101",
            "value": 1175378467
          }
        },
        "99ceee44faf4434fa23fd46e22d4b003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8929d71b71c4603b28d4d16aa14e3e6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0ed503eaad1049fc95b26a373fcacf56",
            "value": "‚Äá1.18G/1.18G‚Äá[00:08&lt;00:00,‚Äá326MB/s]"
          }
        },
        "07e854d90ad94c5e85ab7efc6e8a36e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65df1c9dddbf4665b43d477f26c17cf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b829aeec9314d95b3de85cf23fc77f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "350d036fe5624c08966d85824e602718": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61849b775da848e9ae172c1f12c13101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8929d71b71c4603b28d4d16aa14e3e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ed503eaad1049fc95b26a373fcacf56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}