{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training an Image Classification Model in PyTorch",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f592e77ff03d4ae289be1a9c56e63519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2b9bea1f42b74fd99fcfb433621bbb17",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bd74e94e35cb459aa710421e68e70c19",
              "IPY_MODEL_abb35d9f2b884fbd8157d8b18cd37246",
              "IPY_MODEL_cc727e87b211439a8f11ee10ed67d0f5"
            ]
          }
        },
        "2b9bea1f42b74fd99fcfb433621bbb17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd74e94e35cb459aa710421e68e70c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dfe1a467cb7f4367b6d7a9a597b6db48",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b530f8d4c3e149cc8b31955321152384"
          }
        },
        "abb35d9f2b884fbd8157d8b18cd37246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_34c62e75793642ecb66ae0cb39cd5c92",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46830571,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46830571,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b9200682d0e84753a7007661c9649c81"
          }
        },
        "cc727e87b211439a8f11ee10ed67d0f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_782e6640408d46ff8a8f24aa8b3cb8dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 71.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e446070a5e8d4d309b74b97b0446e50d"
          }
        },
        "dfe1a467cb7f4367b6d7a9a597b6db48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b530f8d4c3e149cc8b31955321152384": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "34c62e75793642ecb66ae0cb39cd5c92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b9200682d0e84753a7007661c9649c81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "782e6640408d46ff8a8f24aa8b3cb8dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e446070a5e8d4d309b74b97b0446e50d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanggusti/30-days-kaggle/blob/main/colabs/Training_an_Image_Classification_Model_in_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKU8kmSs65xv"
      },
      "source": [
        "# ***Training an Image Classification Model in PyTorch***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zK9b4yiMRzB"
      },
      "source": [
        "#### The primary objective for Hub is to enable users to manage their data more easily so they can train better ML models. This tutorial shows you how to train a simple image classification model while streaming data from a Hub dataset stored in the cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UseHLcoRIYz"
      },
      "source": [
        "## Install Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5mOffq5RN-T"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "!pip3 install hub\n",
        "clear_output()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOkA83IsRWYo"
      },
      "source": [
        "# IMPORTANT - Please restart your Colab runtime after installing Hub!\n",
        "# This is a Colab-specific issue that prevents PIL from working properly.\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wGo53ndMTCB"
      },
      "source": [
        "## Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52h9xKujOJFs"
      },
      "source": [
        "The first step is to select a dataset for training. This tutorial uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset that has already been converted into hub format. It is a simple image classification dataset that categorizes images by clothing type (trouser, shirt, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neD2jhKDQ5WD",
        "outputId": "3950620a-b450-449e-a58c-f486b49ce8b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import hub\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os, time\n",
        "import torch\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "ds_train = hub.load('hub://activeloop/fashion-mnist-train')\n",
        "ds_test = hub.load('hub://activeloop/fashion-mnist-test')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening dataset in read-only mode as you don't have write permissions.\n",
            "hub://activeloop/fashion-mnist-train loaded successfully.\n",
            "Opening dataset in read-only mode as you don't have write permissions.\n",
            "hub://activeloop/fashion-mnist-test loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0jtotSNzeJ0",
        "outputId": "28bbedf0-0569-4ed2-ce5f-9d963ed2bc28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "Image.fromarray(ds_train.images[0].numpy()).resize((100,100))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGQAAABkCAAAAABVicqIAAAOgklEQVR4nO1ayXIkR3J97h6RS22oAtDobm5D2YiiaUwXmUn//xW6SCaTaTQcTg97QQO1ZGaE+9MhC71whsMGTQcdGIcqoDIrXoa7h/t7HgX8On4dv47/F0N+6RdNjeHxwUT8yXvTLwXJy46n/QOIQPDTKI8BEbyfyPrLjb8Zy/mKCCDBH999/sIvWomo9durbY398YM5/y/NJWpNv1jtLlfTcDcFELWQIsIHtIdBQAB5JIggAE3N5up6u1ro6XRkFZZDAch5KYIHB5EAKPI4kPlhRZvl7rMvr1qZ8jhKEQ5SJrz3lswgDx88EuQ8Ure+fPrFdSpHK1WrYMh6536OZs7W+tg/j1qJBiDNand9c3OtQ1aPHEnL5dXt8e5umO8iP4hmmf//dBARFQDarq+vn1zt5KRK6dFkHw9vX//Rp3cb890yVAgSfLS5pFlfXl3tLmAUSo/cMKb9KoZ6rB8aSVISIaMGH+UTggDT8mK33awpIZS2BiPlzBLLwzC5e1QPWEpt14pPUxmLPwKEBAmE9ZvterWMKISm/f2J3bK/ypu7+/1hGIfxeJq0Wy7XFyuc3t4dQD7GXDHvhLzYbFZ9G6WjiIzlzldNv1nfDPe3t/vD/nAnquvd7vLqAnc/NAmEP85cEOmW682mzyY5hyiGNotZajJr3zZd17bJBttcXu52a2RYAqU+zvGa8ma3261aOGFJVCL6KXUaU9QS1oY0/bam5XrZ8ii66xfZ2ulTQebYt3Z1fX21XeWY4JpULS+HaZrKOE3TNFZpbAnR1LQ67Qe9uLjZGJrxcStJi6vnz643ixQBipk1C4nx/u3bYTyNlZozLHetKcrberDtbrcc74s9DqTZff71F9erNgWhEkJoQiteAcuEBENSm5XhufP2+ulO2pjKz4J8VJ/759/+9qubZaNOnPeNwtq1LddjhaCMYwn38AhcXK6++nJ5rLcvfx7k3SCA/vk33z5ZdSZCCMEgHcyrvpbqIhz2d4fTMJUattjutp8/kTy+ehGPM1fePH22S1AAEMEMo5rBcIrEKYsg3EXT4vrZ7sl65LC/f2SqF1VVIemEikBEdL4SAUggkLvlqVTk7dMnq6bu96fyCan+o8oQ0+nUZ6V7iImKQFUQEeeSmBap30zFA2m1u7A4/PDqEI8tWj6djj0zvYSEqqiKKAiSISSkyYtwkqLdqquH+1e3Ax9dGRlBkmSIkBIiKsIIkCQhYgJAKNIusijd4/EgIqoqIiIICSJAipoAxBlEVYQERDW3XaN4NIiamapSNBgRUb24NF1WciYrVTWpRIgWQ+pXi+bRIGo5m+JcvKPWMo1Fe1UwziwrTCgRUosSue/b/Ikg79ihpqZJwoggIaDXMk6WYRoMmWNaGAAkJhLI7WK5/IR6Ih+QYM1tm+l0DyhUGF4rYFkLAFEVBYOiIhxHJtFutb3/pCwswpmDatv2XS4RHqEiFAGD0JQ1BKJqqvDqIqY+FrYN8uLi7fHTfDJTR11uVn2bnDMFnam8mqmqyPw0IhAQmrQc975YBVPbf1LuEhEHtF9dfXa9btMEQqmCmXxrzgJCKQjBjNt2ON69mDZXuTDl/PMglDkX6vLm8y+erFpVDSVEQFHLEilLAAqCDhGIpaYP3v/puMPyVKmPKFrarq8v122GymwcilrWTMvioChJkAIRS8k57e+lL/enyX+eEslDANNLKQGBiEJAEKIeDDUJUAChqipJenU0y4rD8PLN/eFnicTsSRKI4+vF9d0UpjbTCqoIGUHCARElNJs6PUYGLr5YjsP++z+9vP15SiSCOZo43r9+sx9dRQwCEkhJ6FFrCcBEVDSZIKqPhXLZbV5898P3L17efxqDnPl67pd9hmvMygYQNYW5sBICyJw9AwgX2HqV7+vdm7f74cO0Ig9iDJw5AgAwHv7y7Tf/8M3zpkZxigoJRFWJIFTnjCACgCQDmlYSXRyPY/0gd8l5d80gwQed+b4yPv3nf/1q2xeWSp0vR52NKUaCoTyXFQKSevOGpTg+SpDn/SrvVeZHY/XFP/3LjR+nqAFSAEpwjgsxRszrFkQQEE2tdW1WUSszyLspH97Id7JJLaWUzOzq68+vF+O+eLwLB4+HJ4MoVAR0+JxzVCW1y/Wy0Xkl8iAmKRQK8b5jAuhiuV6vln13+bsdynEYeZ4WCI+5/sr5FQjSPQAVuLC7fHZ3e/sehPOG+8uuwmJ7+eTmenexvni6Op1OQ4GqgbODPaChoioqCpAR9ABUlQXeXX1+9+JHReuDlsIcohC5uLx+8uz5k6vLi1XbTHWqATnvm3M7YFbV548AUaGIKlyj3T3989JmEH68ANVkyVSs6bp2e3l5eXm1vbhYdUIPyeYsrkkBQgUE1UxjKkFRy42BBEUkHM1mt108gHxkH81N23ZNtna1u7zYbtfLvuu6LiEikHKMY4GGCURMleGSk4zDfqLmbtP2Sg8PCoN5vV139pc13tqm7RaLZd+kfvv8+dV23WcE1VgjoGbhU4QiRNQ0G9wlZ5RyONE6Re5TVC/VwUCWzap9iC7VnLMKJOWmadquXy67bN326bOrzbJRnyrPseQK1yYiQgxKUdUkOWEyFZjZOdNxTpoprZZnELG02F5fLkwlNzmnlNuuyyLWb9etgeHhJOjVI6htt/LjsUjK4m7SmlkChpKRuzbXk3iNCIpBky0XeQbRtNx9+dvf7BrV3GRTkWQZrNSc6uBZvDqFDC/TabLL9UXgVAALr5pyZwos1rFA05iWGtUJFSPEUt/muUtki93zr7/95kmrmpskCIpo1FI8ECWqRMRZmJa7vdqlWTaFCPiQHcHUJW0ya/FaA5oMCIqmpDNI2n71d7/9zWeXWSQlQ3gAEmZWq8+yg5xJ8HQ6vKi1y52uW4iKmcZEFI86VaRw1qm68+yYiPA4E+588+0/fvn0ojMSwQj3WWISZAgoBFQ1dU3Zn77f7xHXm6vEUkIEcYjpeKq0nLMn1uJByEzJWWUYy6xPuptvfnezyBIRAMPd5xpEzi8EBGK5X5Qu7l4OSaZ8ucRwKKBPp9Pxfj/ZYtW1YeJ1XrSogFFlnCqB1Mr2yWefb1HJIM8CBHPNFouYi4tARDXlfnmIN99F0xHHQyXrdDoN+1PJTTCcQlJFVWc641pDTCM9T188u9o048i5aqnQCIGoiIDu1UsEjCxjtJ+XF7eH379682JntcwGBXWhuV+0yQQwqJiqziQAoqlJJX3ZfX6zzhwPrpZM7Zz3ISKWEuo4jl6cFj5WNF/fvPyPf/vv6Q//uW6SGWBNt+j7vmvmMJpT30zMGEqoNV1b05P+yaYVn4ZqmUyiomcFranJMkUVhiPCg9rdNDfT76fXt68W3WKRRfOSrS03qwYkI85S7JzSCWhq+m5Mlk3C5+5xeKiZ2dx2FTBQy1TqHBB0TciLy+dfLYrlbrlqzfJis1kvF11ihJ/7z7NUPVMLa7p+TKMNx8PRpDEIC6mWk4ioQcDqw/44OUUY8CqSal19s7o9jGg36y6n3PRd0zZKvlcxIhDOIlLE2n5R05H7/f7YIhvDvQZSTSZiluGIctoPFckEQXcyTtb//dfHt3elWa+7JidVIcMJMs7UYG4nzMIb2vQrphNO4zgZCEZEOAA6xBID4eU0jmHnAk36FO1y1Zbb2zEvF12TjKyllggGI4IBBEPO3WF3l2axljRocVHWKQjSDCIRpFpJiKiTU+RBWENURUTzEkXbJgnCaynu1d09GAxATUVmdjlhirzetemkE1UxHVzMLJlEuHtAzQDSaVBVUdVEiKj5qQq7FhKlgKVMDtCnyedtfE4qlppM+FTzatelk42TR52G0CRmWWpErU5Rg4jELP9VdU68Cj8NlrJGFHpEmSZqgo9jnXsVCBJq1gRJKZ76ZUon2d/d3bfMhEa4oE6lelABUYgqVG3OE+c9FjCKuU9Rw+tUISHh8S7xekBTIuhNCoiapWNdvHyxuew7rVMtriy1xszQ7ewCsVmdKwgwPKQW8SisEeExE1aFkgHSS4VFMGoryQxRSzqN7Z//uLJu1Qz7WiOiOmGm5waH6JzEIhiqACJqcBSSQSdBAZ2QBDICIQgnROA1GqjU4XRKBfvXf1zkvjcSdPcaPLfKSPLcraEH5lOHCJ9NI8RcOsC5iRMhmJd+Jn0RZDnt704JGN58x2ncbxvJObyWUklnuKqqmIrOhRaiACQYcW7WQAUqiiDVFFEDYoQ41FLKKfdajm/+/GJMQH2Lu9vXL57dXG1aiTqehhrhFSJzaZhz/vmo6pyeZpL9XqOKCauTYppaQjU1XWsW09s//+H7moDYD69u37x8ddKuTZKUmGqZuz4iOncB3zF5YO4KyoO7HgoDQublJlEA2vat1OH21Yvvv2cC6D7WWorKdNFnEzRWk+MMMjvlASQIURXO/SABiHe6Y+7dQUxZa5SS/fT2xf/8cDsizQJovJM6vf6vi/Vqs7tYZToftI2K4rwSIauHqLB6nDfNh6eKmuYejNTjfgjhcP/6h+/+NADpfMo1vR1uf99uttfPvs67lUHM5iMsfdB4EIFPpUJRx7HMgRYPQhhkOBhBJYbbV7enadi/efVmfze3bAUESjkAWO1u7nS91kZMTFQeXCwIAqCXsUBQTqeJEPfqeGgMEwy6e2ji8eWLV/fHw93r13cEfszq9x6Sdb/JkpquSSYAqWbK4iFgOZ4mCOowFEI8agDvQEBGeKhxePPq9nA87d/u52uJH+mT6Q7T63/vTKxfLJrGJEpo05kPQ4XEuD9OEEYtDiAYD+Y6TxIMirIeD6exTNP56PHHh/5iKeesEF2sV12XxYdq3TKXw3EiYtgfJsh7JXc+Q37/nPM7w32uYn8V5AO41arrstahWL/I5XiYiBgP5afu/1vjp3++IF2Tk3ips7nGSkSJn7z9l4HMwokeYklYPQj4L8L4WyB/5dJP/3jg1/Hr+Mvxv7G/O+UCBivnAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=100x100 at 0x7F3F04ECBE10>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPSz9kml03Aa",
        "outputId": "86bb7115-9378-468c-a4e1-ff81c1df841d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(ds_train.labels.info.class_names[str(ds_train.labels[0].numpy()[0])])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ankle boot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np5fIbViHlCu"
      },
      "source": [
        "The next step is to define a transformation function that will process the data and convert it into a format that can be passed into a deep learning model. The syntax for the transformation function is that the input parameter is a sample from a Hub dataset in dictionary syntax, and the return value is a dictionary containing the data that the training loop uses to train the model. In this particular example, `torchvision.transforms` is used as a part of the transformation pipeline that performs operations such as normalization and image augmentation (rotation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqdWgumwQ1d6"
      },
      "source": [
        "def transform(sample_in):\n",
        "    return {'images': tform(sample_in['images']), 'labels': sample_in['labels']}\n",
        "\n",
        "tform = transforms.Compose([\n",
        "    transforms.ToPILImage(), # Must convert to PIL image for subsequent operations to run\n",
        "    transforms.RandomRotation(20), # Image augmentation\n",
        "    transforms.ToTensor(), # Must convert to pytorch tensor for subsequent operations to run\n",
        "    transforms.Normalize([0.5], [0.5]),\n",
        "])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToNQ3WwfIJZf"
      },
      "source": [
        "**Note:** Don't worry if the above syntax is a bit confusing ðŸ˜µ! We're currently improving it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGmWr44PIQMk"
      },
      "source": [
        "You are now ready to create a pytorch dataloader that connects the Hub dataset to the PyTorch model. This can be done using the provided method `ds.pytorch()` , which automatically applies the user-defined transformation function, takes care of random shuffling (if desired), and converts hub data to PyTorch tensors. The `num_workers` parameter can be used to parallelize data preprocessing, which is critical for ensuring that preprocessing does not bottleneck the overall training workflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeiU4LobROdE",
        "outputId": "badb3b6e-4ffe-4626-ba9c-5ed280355bb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_loader = ds_train.pytorch(num_workers = 2, shuffle = True, transform = transform, batch_size = batch_size)\n",
        "test_loader = ds_test.pytorch(num_workers = 2, transform = transform, batch_size = batch_size)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/hub/integrations/pytorch/pytorch_old.py:86: UserWarning: Python version < 3.8 detected. Pytorch iteration speeds are up to 500% faster on Python version >= 3.8.\n",
            "This version will also not utilize the buffer_size and use_local_cache arguments.\n",
            "Pytorch iteration with shuffling will also be very slow. Use linux/macOS with python >= 3.8 to speed it up.\n",
            "\n",
            "  warnings.warn(warning_message)\n",
            "/usr/local/lib/python3.7/dist-packages/hub/integrations/pytorch/pytorch_old.py:86: UserWarning: Python version < 3.8 detected. Pytorch iteration speeds are up to 500% faster on Python version >= 3.8.\n",
            "This version will also not utilize the buffer_size and use_local_cache arguments.\n",
            "\n",
            "  warnings.warn(warning_message)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Dco8HW9ROXS"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cpu\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snt5b6qwIZQ_"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5LZrDU4I1GO"
      },
      "source": [
        "This tutorial uses a pre-trained [ResNet18](https://pytorch.org/hub/pytorch_vision_resnet/) neural network from the torchvision.models module, converted to a single-channel network for grayscale images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRBRaROLROUf",
        "outputId": "9bca3416-3dd6-47b0-f405-7306e9d0cbe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "f592e77ff03d4ae289be1a9c56e63519",
            "2b9bea1f42b74fd99fcfb433621bbb17",
            "bd74e94e35cb459aa710421e68e70c19",
            "abb35d9f2b884fbd8157d8b18cd37246",
            "cc727e87b211439a8f11ee10ed67d0f5",
            "dfe1a467cb7f4367b6d7a9a597b6db48",
            "b530f8d4c3e149cc8b31955321152384",
            "34c62e75793642ecb66ae0cb39cd5c92",
            "b9200682d0e84753a7007661c9649c81",
            "782e6640408d46ff8a8f24aa8b3cb8dc",
            "e446070a5e8d4d309b74b97b0446e50d"
          ]
        }
      },
      "source": [
        "# Simple model can be trained on a CPU\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "net = models.resnet18(pretrained=True)\n",
        "# Convert model to grayscale\n",
        "net.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Update the fully connected layer based on the number of classes in the dataset\n",
        "net.fc = torch.nn.Linear(net.fc.in_features, len(ds_train.labels.info.class_names))\n",
        "\n",
        "net.to(device)\n",
        "\n",
        "# Specity the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f592e77ff03d4ae289be1a9c56e63519",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sVS5lTFI-gZ"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V65Xr8aBJCUL"
      },
      "source": [
        "Helper functions for training and testing the model are defined. Note that the dictionary that is returned by the transform function in the PyTorch dataloader is access here and is passed into the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6cQJkHeJGtk"
      },
      "source": [
        "def train_model(loader, test_loader, model, epochs = 1):\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "        # Zero the performance stats for each epoch\n",
        "        running_loss = 0.0\n",
        "        start_time = time.time()\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        \n",
        "        for i, data in enumerate(loader):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs = data['images']\n",
        "            labels = torch.squeeze(data['labels'])\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs.float())\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            accuracy = 100 * correct / total\n",
        "        \n",
        "            # Print performance statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 10 == 0:    # print every 10 batches\n",
        "                batch_time = time.time()\n",
        "                speed = (i+1)/(batch_time-start_time)\n",
        "                print('[%d, %5d] loss: %.3f, speed: %.2f, accuracy: %.2f %%' %\n",
        "                      (epoch + 1, i, running_loss, speed, accuracy))\n",
        "\n",
        "                running_loss = 0.0\n",
        "        \n",
        "        print('Testing Model Performance')\n",
        "        test_model(test_loader, model)\n",
        "\n",
        "    print('Finished Training')\n",
        "    \n",
        "    \n",
        "def test_model(loader, model):\n",
        "    start_time = time.time()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(loader):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs = data['images']\n",
        "            labels = torch.squeeze(data['labels'])\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs.float())\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "        print('Finished Testing')\n",
        "        print('Testing accuracy: %.1f %%' %(accuracy))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQWzFjzLJINu"
      },
      "source": [
        "The model and data are ready for training. Let's gooooooooooo ðŸš€!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMhm4VjDRf7i",
        "outputId": "d0ff9c59-9a74-4f40-b40a-cbb5a6b8b62c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_model(train_loader, test_loader, net, epochs = 1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     0] loss: 2.387, speed: 0.29, accuracy: 12.50 %\n",
            "[1,    10] loss: 23.711, speed: 1.35, accuracy: 15.62 %\n",
            "[1,    20] loss: 21.321, speed: 1.64, accuracy: 19.79 %\n",
            "[1,    30] loss: 20.004, speed: 1.80, accuracy: 23.49 %\n",
            "[1,    40] loss: 18.154, speed: 1.89, accuracy: 26.07 %\n",
            "[1,    50] loss: 17.270, speed: 1.94, accuracy: 29.11 %\n",
            "[1,    60] loss: 15.791, speed: 1.97, accuracy: 32.84 %\n",
            "[1,    70] loss: 15.469, speed: 2.00, accuracy: 35.30 %\n",
            "[1,    80] loss: 14.798, speed: 2.03, accuracy: 36.81 %\n",
            "[1,    90] loss: 13.599, speed: 2.04, accuracy: 39.53 %\n",
            "[1,   100] loss: 13.529, speed: 2.06, accuracy: 41.34 %\n",
            "[1,   110] loss: 13.419, speed: 2.07, accuracy: 42.37 %\n",
            "[1,   120] loss: 12.904, speed: 2.08, accuracy: 43.52 %\n",
            "[1,   130] loss: 11.924, speed: 2.09, accuracy: 44.75 %\n",
            "[1,   140] loss: 11.560, speed: 2.09, accuracy: 45.94 %\n",
            "[1,   150] loss: 11.439, speed: 2.10, accuracy: 46.88 %\n",
            "[1,   160] loss: 12.145, speed: 2.10, accuracy: 47.50 %\n",
            "[1,   170] loss: 10.894, speed: 2.10, accuracy: 48.45 %\n",
            "[1,   180] loss: 10.340, speed: 2.11, accuracy: 49.31 %\n",
            "[1,   190] loss: 11.021, speed: 2.11, accuracy: 49.97 %\n",
            "[1,   200] loss: 10.382, speed: 2.11, accuracy: 50.86 %\n",
            "[1,   210] loss: 12.062, speed: 2.12, accuracy: 51.13 %\n",
            "[1,   220] loss: 10.369, speed: 2.12, accuracy: 51.67 %\n",
            "[1,   230] loss: 10.987, speed: 2.12, accuracy: 52.16 %\n",
            "[1,   240] loss: 11.541, speed: 2.13, accuracy: 52.48 %\n",
            "[1,   250] loss: 10.696, speed: 2.13, accuracy: 52.85 %\n",
            "[1,   260] loss: 10.895, speed: 2.13, accuracy: 53.16 %\n",
            "[1,   270] loss: 9.639, speed: 2.13, accuracy: 53.66 %\n",
            "[1,   280] loss: 9.247, speed: 2.13, accuracy: 54.17 %\n",
            "[1,   290] loss: 9.551, speed: 2.13, accuracy: 54.65 %\n",
            "[1,   300] loss: 10.133, speed: 2.14, accuracy: 55.00 %\n",
            "[1,   310] loss: 10.825, speed: 2.14, accuracy: 55.15 %\n",
            "[1,   320] loss: 9.653, speed: 2.14, accuracy: 55.56 %\n",
            "[1,   330] loss: 10.044, speed: 2.14, accuracy: 55.74 %\n",
            "[1,   340] loss: 8.775, speed: 2.14, accuracy: 56.09 %\n",
            "[1,   350] loss: 9.307, speed: 2.14, accuracy: 56.45 %\n",
            "[1,   360] loss: 8.885, speed: 2.14, accuracy: 56.80 %\n",
            "[1,   370] loss: 9.110, speed: 2.14, accuracy: 57.04 %\n",
            "[1,   380] loss: 8.588, speed: 2.15, accuracy: 57.42 %\n",
            "[1,   390] loss: 8.667, speed: 2.15, accuracy: 57.74 %\n",
            "[1,   400] loss: 8.571, speed: 2.15, accuracy: 58.00 %\n",
            "[1,   410] loss: 9.625, speed: 2.15, accuracy: 58.13 %\n",
            "[1,   420] loss: 9.178, speed: 2.14, accuracy: 58.34 %\n",
            "[1,   430] loss: 8.918, speed: 2.15, accuracy: 58.54 %\n",
            "[1,   440] loss: 8.615, speed: 2.14, accuracy: 58.75 %\n",
            "[1,   450] loss: 9.860, speed: 2.15, accuracy: 58.92 %\n",
            "[1,   460] loss: 9.702, speed: 2.15, accuracy: 59.10 %\n",
            "[1,   470] loss: 8.550, speed: 2.15, accuracy: 59.33 %\n",
            "[1,   480] loss: 8.869, speed: 2.15, accuracy: 59.54 %\n",
            "[1,   490] loss: 7.711, speed: 2.15, accuracy: 59.80 %\n",
            "[1,   500] loss: 8.313, speed: 2.15, accuracy: 59.99 %\n",
            "[1,   510] loss: 8.859, speed: 2.15, accuracy: 60.10 %\n",
            "[1,   520] loss: 8.641, speed: 2.15, accuracy: 60.28 %\n",
            "[1,   530] loss: 8.689, speed: 2.15, accuracy: 60.45 %\n",
            "[1,   540] loss: 7.822, speed: 2.15, accuracy: 60.68 %\n",
            "[1,   550] loss: 7.919, speed: 2.15, accuracy: 60.87 %\n",
            "[1,   560] loss: 8.158, speed: 2.15, accuracy: 61.03 %\n",
            "[1,   570] loss: 8.705, speed: 2.15, accuracy: 61.13 %\n",
            "[1,   580] loss: 8.582, speed: 2.15, accuracy: 61.26 %\n",
            "[1,   590] loss: 8.161, speed: 2.15, accuracy: 61.42 %\n",
            "[1,   600] loss: 7.584, speed: 2.15, accuracy: 61.59 %\n",
            "[1,   610] loss: 7.088, speed: 2.15, accuracy: 61.81 %\n",
            "[1,   620] loss: 7.141, speed: 2.16, accuracy: 62.05 %\n",
            "[1,   630] loss: 7.385, speed: 2.16, accuracy: 62.27 %\n",
            "[1,   640] loss: 8.342, speed: 2.16, accuracy: 62.39 %\n",
            "[1,   650] loss: 7.869, speed: 2.16, accuracy: 62.56 %\n",
            "[1,   660] loss: 8.100, speed: 2.16, accuracy: 62.63 %\n",
            "[1,   670] loss: 8.185, speed: 2.16, accuracy: 62.78 %\n",
            "[1,   680] loss: 9.222, speed: 2.16, accuracy: 62.86 %\n",
            "[1,   690] loss: 7.744, speed: 2.16, accuracy: 63.00 %\n",
            "[1,   700] loss: 8.615, speed: 2.16, accuracy: 63.10 %\n",
            "[1,   710] loss: 7.232, speed: 2.16, accuracy: 63.21 %\n",
            "[1,   720] loss: 7.564, speed: 2.16, accuracy: 63.35 %\n",
            "[1,   730] loss: 7.210, speed: 2.16, accuracy: 63.52 %\n",
            "[1,   740] loss: 7.957, speed: 2.16, accuracy: 63.60 %\n",
            "[1,   750] loss: 7.477, speed: 2.16, accuracy: 63.70 %\n",
            "[1,   760] loss: 8.526, speed: 2.16, accuracy: 63.78 %\n",
            "[1,   770] loss: 6.840, speed: 2.16, accuracy: 63.91 %\n",
            "[1,   780] loss: 7.780, speed: 2.16, accuracy: 64.01 %\n",
            "[1,   790] loss: 8.541, speed: 2.16, accuracy: 64.06 %\n",
            "[1,   800] loss: 8.020, speed: 2.16, accuracy: 64.13 %\n",
            "[1,   810] loss: 8.009, speed: 2.16, accuracy: 64.21 %\n",
            "[1,   820] loss: 8.063, speed: 2.16, accuracy: 64.30 %\n",
            "[1,   830] loss: 6.182, speed: 2.16, accuracy: 64.45 %\n",
            "[1,   840] loss: 7.400, speed: 2.16, accuracy: 64.53 %\n",
            "[1,   850] loss: 6.195, speed: 2.16, accuracy: 64.67 %\n",
            "[1,   860] loss: 8.634, speed: 2.16, accuracy: 64.70 %\n",
            "[1,   870] loss: 7.641, speed: 2.16, accuracy: 64.76 %\n",
            "[1,   880] loss: 7.549, speed: 2.16, accuracy: 64.88 %\n",
            "[1,   890] loss: 7.240, speed: 2.16, accuracy: 64.96 %\n",
            "[1,   900] loss: 7.139, speed: 2.16, accuracy: 65.04 %\n",
            "[1,   910] loss: 6.570, speed: 2.16, accuracy: 65.18 %\n",
            "[1,   920] loss: 7.347, speed: 2.17, accuracy: 65.24 %\n",
            "[1,   930] loss: 6.800, speed: 2.17, accuracy: 65.35 %\n",
            "[1,   940] loss: 7.568, speed: 2.17, accuracy: 65.44 %\n",
            "[1,   950] loss: 6.293, speed: 2.17, accuracy: 65.58 %\n",
            "[1,   960] loss: 7.946, speed: 2.17, accuracy: 65.64 %\n",
            "[1,   970] loss: 7.087, speed: 2.17, accuracy: 65.70 %\n",
            "[1,   980] loss: 7.553, speed: 2.17, accuracy: 65.79 %\n",
            "[1,   990] loss: 7.104, speed: 2.17, accuracy: 65.88 %\n",
            "[1,  1000] loss: 7.767, speed: 2.17, accuracy: 65.96 %\n",
            "[1,  1010] loss: 7.744, speed: 2.17, accuracy: 65.99 %\n",
            "[1,  1020] loss: 7.821, speed: 2.17, accuracy: 66.04 %\n",
            "[1,  1030] loss: 7.551, speed: 2.17, accuracy: 66.12 %\n",
            "[1,  1040] loss: 7.623, speed: 2.17, accuracy: 66.17 %\n",
            "[1,  1050] loss: 7.867, speed: 2.17, accuracy: 66.22 %\n",
            "[1,  1060] loss: 7.116, speed: 2.17, accuracy: 66.31 %\n",
            "[1,  1070] loss: 6.890, speed: 2.17, accuracy: 66.40 %\n",
            "[1,  1080] loss: 7.489, speed: 2.17, accuracy: 66.45 %\n",
            "[1,  1090] loss: 7.201, speed: 2.17, accuracy: 66.50 %\n",
            "[1,  1100] loss: 6.658, speed: 2.17, accuracy: 66.58 %\n",
            "[1,  1110] loss: 6.690, speed: 2.17, accuracy: 66.66 %\n",
            "[1,  1120] loss: 7.440, speed: 2.17, accuracy: 66.70 %\n",
            "[1,  1130] loss: 6.267, speed: 2.17, accuracy: 66.79 %\n",
            "[1,  1140] loss: 6.829, speed: 2.17, accuracy: 66.85 %\n",
            "[1,  1150] loss: 7.136, speed: 2.17, accuracy: 66.92 %\n",
            "[1,  1160] loss: 6.907, speed: 2.17, accuracy: 66.99 %\n",
            "[1,  1170] loss: 7.756, speed: 2.17, accuracy: 67.03 %\n",
            "[1,  1180] loss: 7.396, speed: 2.17, accuracy: 67.09 %\n",
            "[1,  1190] loss: 7.052, speed: 2.17, accuracy: 67.15 %\n",
            "[1,  1200] loss: 6.404, speed: 2.17, accuracy: 67.21 %\n",
            "[1,  1210] loss: 6.828, speed: 2.17, accuracy: 67.27 %\n",
            "[1,  1220] loss: 7.698, speed: 2.17, accuracy: 67.33 %\n",
            "[1,  1230] loss: 7.040, speed: 2.17, accuracy: 67.40 %\n",
            "[1,  1240] loss: 6.388, speed: 2.17, accuracy: 67.47 %\n",
            "[1,  1250] loss: 7.520, speed: 2.17, accuracy: 67.49 %\n",
            "[1,  1260] loss: 8.555, speed: 2.18, accuracy: 67.50 %\n",
            "[1,  1270] loss: 7.147, speed: 2.18, accuracy: 67.56 %\n",
            "[1,  1280] loss: 6.781, speed: 2.18, accuracy: 67.63 %\n",
            "[1,  1290] loss: 6.517, speed: 2.18, accuracy: 67.68 %\n",
            "[1,  1300] loss: 6.752, speed: 2.18, accuracy: 67.75 %\n",
            "[1,  1310] loss: 7.049, speed: 2.18, accuracy: 67.80 %\n",
            "[1,  1320] loss: 7.090, speed: 2.18, accuracy: 67.86 %\n",
            "[1,  1330] loss: 7.488, speed: 2.18, accuracy: 67.89 %\n",
            "[1,  1340] loss: 7.600, speed: 2.18, accuracy: 67.92 %\n",
            "[1,  1350] loss: 5.870, speed: 2.18, accuracy: 68.01 %\n",
            "[1,  1360] loss: 6.576, speed: 2.18, accuracy: 68.05 %\n",
            "[1,  1370] loss: 7.009, speed: 2.18, accuracy: 68.11 %\n",
            "[1,  1380] loss: 7.223, speed: 2.18, accuracy: 68.15 %\n",
            "[1,  1390] loss: 5.904, speed: 2.18, accuracy: 68.22 %\n",
            "[1,  1400] loss: 6.179, speed: 2.18, accuracy: 68.29 %\n",
            "[1,  1410] loss: 7.119, speed: 2.18, accuracy: 68.37 %\n",
            "[1,  1420] loss: 7.582, speed: 2.18, accuracy: 68.39 %\n",
            "[1,  1430] loss: 6.856, speed: 2.18, accuracy: 68.46 %\n",
            "[1,  1440] loss: 7.369, speed: 2.18, accuracy: 68.48 %\n",
            "[1,  1450] loss: 6.386, speed: 2.18, accuracy: 68.53 %\n",
            "[1,  1460] loss: 7.071, speed: 2.18, accuracy: 68.57 %\n",
            "[1,  1470] loss: 6.099, speed: 2.18, accuracy: 68.62 %\n",
            "[1,  1480] loss: 6.839, speed: 2.18, accuracy: 68.67 %\n",
            "[1,  1490] loss: 6.955, speed: 2.18, accuracy: 68.72 %\n",
            "[1,  1500] loss: 6.650, speed: 2.18, accuracy: 68.79 %\n",
            "[1,  1510] loss: 6.620, speed: 2.18, accuracy: 68.84 %\n",
            "[1,  1520] loss: 5.969, speed: 2.19, accuracy: 68.89 %\n",
            "[1,  1530] loss: 6.410, speed: 2.19, accuracy: 68.94 %\n",
            "[1,  1540] loss: 6.521, speed: 2.19, accuracy: 68.98 %\n",
            "[1,  1550] loss: 6.631, speed: 2.19, accuracy: 69.02 %\n",
            "[1,  1560] loss: 6.920, speed: 2.19, accuracy: 69.08 %\n",
            "[1,  1570] loss: 6.703, speed: 2.19, accuracy: 69.11 %\n",
            "[1,  1580] loss: 6.181, speed: 2.19, accuracy: 69.16 %\n",
            "[1,  1590] loss: 6.368, speed: 2.19, accuracy: 69.21 %\n",
            "[1,  1600] loss: 6.841, speed: 2.19, accuracy: 69.26 %\n",
            "[1,  1610] loss: 5.935, speed: 2.19, accuracy: 69.31 %\n",
            "[1,  1620] loss: 6.980, speed: 2.19, accuracy: 69.36 %\n",
            "[1,  1630] loss: 6.868, speed: 2.19, accuracy: 69.38 %\n",
            "[1,  1640] loss: 5.420, speed: 2.19, accuracy: 69.45 %\n",
            "[1,  1650] loss: 6.258, speed: 2.19, accuracy: 69.50 %\n",
            "[1,  1660] loss: 6.623, speed: 2.19, accuracy: 69.54 %\n",
            "[1,  1670] loss: 6.948, speed: 2.19, accuracy: 69.56 %\n",
            "[1,  1680] loss: 6.866, speed: 2.19, accuracy: 69.60 %\n",
            "[1,  1690] loss: 6.020, speed: 2.19, accuracy: 69.65 %\n",
            "[1,  1700] loss: 5.825, speed: 2.19, accuracy: 69.70 %\n",
            "[1,  1710] loss: 6.655, speed: 2.19, accuracy: 69.74 %\n",
            "[1,  1720] loss: 6.157, speed: 2.19, accuracy: 69.79 %\n",
            "[1,  1730] loss: 6.186, speed: 2.19, accuracy: 69.83 %\n",
            "[1,  1740] loss: 6.103, speed: 2.19, accuracy: 69.88 %\n",
            "[1,  1750] loss: 6.228, speed: 2.19, accuracy: 69.91 %\n",
            "[1,  1760] loss: 6.118, speed: 2.19, accuracy: 69.96 %\n",
            "[1,  1770] loss: 7.189, speed: 2.19, accuracy: 69.98 %\n",
            "[1,  1780] loss: 6.069, speed: 2.19, accuracy: 70.02 %\n",
            "[1,  1790] loss: 7.048, speed: 2.19, accuracy: 70.03 %\n",
            "[1,  1800] loss: 6.811, speed: 2.19, accuracy: 70.07 %\n",
            "[1,  1810] loss: 7.075, speed: 2.19, accuracy: 70.09 %\n",
            "[1,  1820] loss: 5.818, speed: 2.19, accuracy: 70.14 %\n",
            "[1,  1830] loss: 6.909, speed: 2.19, accuracy: 70.16 %\n",
            "[1,  1840] loss: 5.820, speed: 2.19, accuracy: 70.20 %\n",
            "[1,  1850] loss: 6.370, speed: 2.19, accuracy: 70.26 %\n",
            "[1,  1860] loss: 6.462, speed: 2.19, accuracy: 70.29 %\n",
            "[1,  1870] loss: 6.926, speed: 2.19, accuracy: 70.30 %\n",
            "Testing Model Performance\n",
            "Finished Testing\n",
            "Testing accuracy: 76.1 %\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79QnkE-UUySP"
      },
      "source": [
        "Congrats! You successfully trained an image classification model while streaming data directly from the cloud! ðŸŽ‰"
      ]
    }
  ]
}